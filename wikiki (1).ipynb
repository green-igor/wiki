{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn -q\n",
    "#!pip install swifter - q\n",
    "\n",
    "#!pip install lightgbm\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import re\n",
    "from time import time \n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "from sklearn.model_selection import FixedThresholdClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\igor.grenaderov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#from spacy.pipeline import Lemmatizer\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.BertTokenizer(\n",
    "#    vocab_file='datasets/ds_bert/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = English()\n",
    "#nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2002, 2743, 1012, 2002, 3062, 1012, 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('He ran. He fell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102, '[MASK]': 103}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, '[CLS]')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2002, 'he')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2743, 'ran')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1012, '.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2002, 'he')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3062, 'fell')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1012, '.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(13360, 'aaa')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11057, '##aa')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11057, '##aa')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11057, '##aa')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11057, '##aa')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1012, '.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1012, '.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1012, '.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(102, '[SEP]')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for enc in tokenizer.encode('He ran. He fell. Aaaaaaaaaaa...'):\n",
    "    display((enc,tokenizer.decode([f'{enc}'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "#df['text'].apply(lambda x: len(str(x).split()))\n",
    "end = time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://code.s3.yandex.net/datasets/toxic_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_text_tok(string):\n",
    "\n",
    "    string = string.strip()\n",
    "    string = re.sub(r'\\[?https?://\\S+\\]?|\\[?www\\.\\S+\\]?', 'lnkt', string)\n",
    "    string = re.sub(r'\\[{1,2}.+\\]{1,2}', 'rbrct', string)\n",
    "    string = re.sub(r'\\{{1,2}.+\\}{1,2}', 'brct', string)\n",
    "    \n",
    "    string = re.sub('\\\\d+\\\\.?', 'dgtl', string)\n",
    "    \n",
    "    \n",
    "    string = re.sub(r'\\n+(?=[a-zA-Z])', ' . ', string) #' '\n",
    "\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "       \n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####df.drop_duplicates(subset='clean_text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['clean_text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "####df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0d47ee02024c4b83ff3f5dabf73535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['word_count'] = df['text'].swifter.apply(lambda x: x.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83d7abe97b24b1882beab155b146e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['sent_count'] = df['text'].swifter.apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08bef9362d449628a565e958e6036d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['len'] = df['text'].swifter.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[df['len']==df['len'].max()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   text        159292 non-null  object\n",
      " 1   toxic       159292 non-null  int64 \n",
      " 2   word_count  159292 non-null  int64 \n",
      " 3   sent_count  159292 non-null  int64 \n",
      " 4   len         159292 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def chunker(string):\n",
    "\n",
    "    string = clean_text(string)\n",
    "    \n",
    "    sents_ = sent_tokenize(string)\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(sent.strip(), add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    usents=[]\n",
    "    \n",
    "    for sent in tokenized_sents:\n",
    "        if sent not in usents and sent!=[101,1012,102]:\n",
    "            usents.append(sent)\n",
    "    \n",
    "    if len(usents)>1:\n",
    "       result =  sum(usents, [])\n",
    "    \n",
    "    elif len(usents)==1:\n",
    "        result = usents[0]\n",
    "    \n",
    "    else:\n",
    "        result = tokenizer.encode('rubbish', add_special_tokens=True)#tokenized_sents[0]\n",
    "\n",
    "    if len(set(result))/len(result)<0.1:\n",
    "        result = list(dict.fromkeys(result[:-1]))#[:-1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return [101] + result[1:-1][:510] + [102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_split(string):\n",
    "    string = re.split('\\n+',string)\n",
    "    result = [sent_tokenize(i.strip()) for i in string]\n",
    "    return sum(result,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "\n",
    "    string = string.strip()\n",
    "    string = re.sub(r'\\[?https?://\\S+\\]?|\\[?www\\.\\S+\\]?', ' [lnkt] ', string)\n",
    "    string = re.sub(r'\\[{1,2}.+\\]{1,2}', ' [brct] ', string)\n",
    "    string = re.sub(r'\\{{1,2}.+\\}{1,2}', ' {rbrct} ', string)\n",
    "    #string = re.sub('\\\\d+\\\\.?', ' dgtl ', string)\n",
    "    \n",
    "    string = re.sub(r'[\\_\\+]',' ',string)\n",
    "    string = re.sub(r'[^a-zA-Z0-9\\s!?.,\\[\\]\\{\\}\\(\\)\\:\\;]',' ',string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "\n",
    "    \n",
    "       \n",
    "    return string.strip()#.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(string):\n",
    "\n",
    "    sents_ = nl_split(string)\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(clean_text(sent), add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    usents=[]\n",
    "    \n",
    "    for sent in tokenized_sents:\n",
    "        if sent not in usents and sent!=[101,1012,102] and sent!=[101,102]:\n",
    "            usents.append(sent)\n",
    "    \n",
    "    if len(usents)>1:\n",
    "       result =  sum(usents, [])\n",
    "    \n",
    "    elif len(usents)==1:\n",
    "        result = usents[0]\n",
    "    \n",
    "    else:\n",
    "        result = tokenizer.encode(sents_[0])\n",
    "\n",
    "    if len(set(result))/len(string)<0.02:\n",
    "        result = list(dict.fromkeys(result))\n",
    "        \n",
    "    if len(result)>512:\n",
    "        result = result[:511]+[102]\n",
    "    \n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.decode(chunker(df['text'][96905]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.decode(chunker(df['text'][4704]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.decode(chunker(df['text'][82496]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc9a88bc7b34455a6749123302e1485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "df['after_tok'] = df['text'].swifter.apply(lambda x: tokenizer.decode(chunker(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a_t_len'] = df['after_tok'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['after_tok'].duplicated().sum() #1194 # 742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[['after_tok','text','toxic']].sort_values(by='after_tok').iloc[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='after_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158572 entries, 0 to 158571\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   text        158572 non-null  object\n",
      " 1   toxic       158572 non-null  int64 \n",
      " 2   word_count  158572 non-null  int64 \n",
      " 3   sent_count  158572 non-null  int64 \n",
      " 4   len         158572 non-null  int64 \n",
      " 5   after_tok   158572 non-null  object\n",
      " 6   a_t_len     158572 non-null  int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plt(df_list, name, val_list, plt_xlim=1):\n",
    "\n",
    "    x_max = min([series.max() for series in df_list])* plt_xlim\n",
    "\n",
    "    fig, ax = plt.subplots(2,1,figsize=(10,8))\n",
    "    ax[0].boxplot([df for df in df_list], vert=False)\n",
    "\n",
    "    ax[0].set_yticklabels(val_list)\n",
    "    ax[0].set_xlabel(name)\n",
    "    ax[0].set_xlim([0, x_max])\n",
    "\n",
    "    ax[1].hist([df for df in df_list], histtype='stepfilled', bins=500)\n",
    "    ax[1].set_ylabel('частота')\n",
    "    ax[1].set_xlabel(name)\n",
    "    ax[1].set_xlim([0, x_max])\n",
    "\n",
    "    \n",
    "    plt.suptitle(f'Ящики с усами и гистограмма для признака \"{name}\"')\n",
    "    plt.legend(val_list);\n",
    "    #plt.xlim((0,x_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {'word_count' : 'количество слов',\n",
    " 'sent_count' : 'количество предложений',\n",
    " 'len' : 'количество символов'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for col in ['word_count', 'sent_count', 'len']:\n",
    "    \n",
    "    distribution_plt(\n",
    "        \n",
    "        [df[df['toxic']==1][col], df[df['toxic']==0][col]],\n",
    "        \n",
    "        name_dict[col],\n",
    "        \n",
    "        ['токсичные\\nкомментарии', 'не токсичные\\nкомментарии'],\n",
    "        \n",
    "        0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logy(feats, target):\n",
    "    lr = LogisticRegression(random_state=RANDOM_STATE,max_iter=1000,class_weight='balanced')\n",
    "    lr.fit(feats, target)\n",
    "    scores = cross_val_score(lr, feats, target, cv=5, scoring='f1_macro')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158572, 7)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df#.sample(n=60000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    samp['text'],\n",
    "    samp['toxic'],\n",
    "    test_size = 0.17,\n",
    "    random_state = RANDOM_STATE,\n",
    "    stratify = samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131614,), (26958,), (131614,), (26958,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_resample, y_resample = sampler.fit_resample(pd.DataFrame(X_train), pd.DataFrame(y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26686, 1), (26686, 1))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resample.shape, y_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = X_resample.join(y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_samp = res_df.sample(n=10000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samp = pd.DataFrame(X_test).join(pd.DataFrame(y_test)).sample(n=10000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_samp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_resample.shape, y_resample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def chunker(string):\n",
    "\n",
    "    string = clean_text(string) \n",
    "    \n",
    "    sents_ = sent_tokenize(string)\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(sent.strip(), add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    usents=[]\n",
    "    \n",
    "    for sent in tokenized_sents:\n",
    "        if sent not in usents and sent!=[101,1012,102]:\n",
    "            usents.append(sent)\n",
    "    \n",
    "    if len(usents)>1:\n",
    "       result =  sum(usents, [])\n",
    "    elif len(usents)==1:\n",
    "        result = usents[0]\n",
    "    else:\n",
    "        result = tokenized_sents[0]\n",
    "\n",
    "    if len(set(result))/len(result)<0.1:\n",
    "        result = list(dict.fromkeys(result[:-1]))#[:-1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    return [101] + result[1:-1][:510] + [102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def chunker(string):\n",
    "    \n",
    "    sents_ = sents(string)\n",
    "    \n",
    "    tokenz = tokenizer(sents_, add_special_tokens=True, padding=True, return_tensors='np')\n",
    "\n",
    "\n",
    "    model = transformers.BertModel.from_pretrained('bert-large-uncased')#'distilbert-base-uncased')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "                    batch_embeddings = model(torch.LongTensor(tokenz['input_ids']), attention_mask=torch.LongTensor(tokenz['attention_mask']))\n",
    "    #transformers.from_pretrained(distilbert-base-uncased)\n",
    "    \n",
    "    #batch_embeddings[0][:,0,:]\n",
    "    \n",
    "    return np.mean(batch_embeddings[0][:,0,:].numpy(),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #tokenizer,\n",
    "        #text=[],\n",
    "        text,\n",
    "        batch_size=1):\n",
    "        \n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        #self.model_class ,self.pretrained_weights = (\n",
    "        #    transformers.BertModel, 'bert-base-uncased')\n",
    "        self.model_class ,self.pretrained_weights = (\n",
    "            transformers.DistilBertModel, 'distilbert-base-uncased')\n",
    "\n",
    "    \n",
    "        \n",
    "        self.model = self.model_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "    \n",
    "    def get(self):\n",
    "\n",
    "        \n",
    "        tokenized = self.text.swifter.apply(lambda x: chunker(x))\n",
    "        \n",
    "        max_len = 0\n",
    "        \n",
    "        for tok in tokenized.values:\n",
    "            if len(tok) > max_len:\n",
    "                max_len = len(tok)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        \n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for i in notebook.tqdm(range(len(padded) // batch_size)):\n",
    "                batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "                attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.model(batch, attention_mask=attention_mask_batch)\n",
    "                \n",
    "                embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "    \n",
    "        features = np.concatenate(embeddings)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[df['text'].str.contains('I ass. I ass.')]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker(df['text'][4703]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker(df['text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker('someone needs to remove lott 71. 227. 167. 147'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer(df['text'][0:2]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_samp['text'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5198993936a84f1e8a0e0a657d452a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea42a875719d468a8128546cb2504340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dembs_train = BertTokenizer(text=res_samp['text'], batch_size=50).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8919973 , 0.8999975 , 0.90249588, 0.89397286, 0.89449786])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logy(dembs_train, res_samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([0.90399962, 0.88947345, 0.87997972, 0.8889929 , 0.89399958])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([0.90499948, 0.89499859, 0.89465875, 0.89533315, 0.89899909]) #last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897797812"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.90499948, 0.89499859, 0.89465875, 0.89533315, 0.89899909]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8978900940000001"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.90749332, 0.8999999 , 0.89248815, 0.88999989, 0.89946921]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.80789152, 0.80608067, 0.80544205, 0.79700029, 0.81400535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcded7b9f15142ce8132eea5e794255e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999526c0fc204e499acca96d35f3ab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dembs_test = BertTokenizer(text=test_samp['text'], batch_size=50).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55718584, 0.55718584, 0.55718584, ..., 0.55718584, 4.87171159,\n",
       "       0.55718584], shape=(30000,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train \n",
    ")\n",
    "\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    0.89862\n",
       "1    0.10138\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y_true, y_pred,):\n",
    "    #y_true = dtrain.get_label()\n",
    "    #err = 1 - f1_score(grid_pred, y_test, average=None)[1]\n",
    "    err = f1_score(y_pred, y_true, average=None)[1]\n",
    "    return  err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_xgb(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err =1 - f1_score(y_true, y_pred, average=None)[1]\n",
    "\n",
    "    return 'err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "my_func = make_scorer(f1_eval, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    0.89862\n",
       "1    0.10138\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debs_no_dups = pd.DataFrame(dembs_train).join(pd.DataFrame(res_samp.reset_index()['toxic']))\n",
    "#dembs_train, res_samp['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debs_no_dups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 769)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debs_no_dups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debs_no_dups.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "debs_no_dups.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 769)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debs_no_dups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('models',[LogisticRegression(random_state=RANDOM_STATE,class_weight='balanced')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__solver': ['newton-cg', 'liblinear'],\n",
    "        #'models__class_weight':,\n",
    "        #'models__C': [0.1, 1]#,\n",
    "        #'models__penalty': ['l1', 'l2']\n",
    "          \n",
    "    },\n",
    "\n",
    "#    {\n",
    "#        'models': [SVC(random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "#        'models__degree': range(3,8),\n",
    "#        'models__kernel': ['poly','rbf','sigmoid']\n",
    "#    },\n",
    "    {\n",
    "        'models': [XGBClassifier(\n",
    "            random_state=RANDOM_STATE#,\n",
    "            #scale_pos_weight = 0.897367\n",
    "            \n",
    "            )],\n",
    "        'models__n_estimators':[500,700],\n",
    "        'models__max_depth':[8,9],\n",
    "        'models__learning_rate':[0.7,0.1],\n",
    "        'models__eval_metric': [f1_xgb]\n",
    "    }\n",
    "    #{\n",
    "    #    'models': [LGBMClassifier(random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "    #    'models__n_estimators': [1000],\n",
    "    #    'models__learning_rate': [0.1, 0.01, 0.03, 0.05, 0.07]\n",
    "    #}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__solver': ['newton-cg', 'liblinear'],\n",
    "        #'models__class_weight':,\n",
    "        'models__C': [0.1,0.5,1,3,7],\n",
    "        'models__penalty': ['l1', 'l2']\n",
    "          \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipe, \n",
    "    param_grid, \n",
    "    cv=5,\n",
    "    scoring=my_func,\n",
    "    #scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(dembs_train, res_samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_samp['toxic'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.897591</td>\n",
       "      <td>{'models': LogisticRegression(class_weight='ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.896625</td>\n",
       "      <td>{'models': LogisticRegression(class_weight='ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.887530</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0.887089</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.885940</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.881217</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.880460</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.878960</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.878328</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, boos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score  mean_test_score  \\\n",
       "0                1         0.897591   \n",
       "1                2         0.896625   \n",
       "7                3         0.887530   \n",
       "9                4         0.887089   \n",
       "6                5         0.885940   \n",
       "8                6         0.884259   \n",
       "5                7         0.881217   \n",
       "4                8         0.880460   \n",
       "3                9         0.878960   \n",
       "2               10         0.878328   \n",
       "\n",
       "                                              params  \n",
       "0  {'models': LogisticRegression(class_weight='ba...  \n",
       "1  {'models': LogisticRegression(class_weight='ba...  \n",
       "7  {'models': XGBClassifier(base_score=None, boos...  \n",
       "9  {'models': XGBClassifier(base_score=None, boos...  \n",
       "6  {'models': XGBClassifier(base_score=None, boos...  \n",
       "8  {'models': XGBClassifier(base_score=None, boos...  \n",
       "5  {'models': XGBClassifier(base_score=None, boos...  \n",
       "4  {'models': XGBClassifier(base_score=None, boos...  \n",
       "3  {'models': XGBClassifier(base_score=None, boos...  \n",
       "2  {'models': XGBClassifier(base_score=None, boos...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'mean_test_score','params']].sort_values('rank_test_score')#.iloc[3]['params']#.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_pred = grid_search.best_estimator_.predict(dembs_test)#[:X_train.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = pd.DataFrame(grid_search.best_estimator_.predict_proba(dembs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94364974, 0.64398097])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(grid_pred, test_samp['toxic'],average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26958,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 768)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dembs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      8258\n",
      "           1       0.89      0.51      0.64      1742\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.75      0.79     10000\n",
      "weighted avg       0.90      0.90      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_pred, test_samp['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_tuned = TunedThresholdClassifierCV(\n",
    "    grid_search.best_estimator_, scoring=my_func,\n",
    ").fit(dembs_train, res_samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_preds = classifier_tuned.predict(dembs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465612357484369"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ct_preds, test_samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5050514581892461"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_tuned.best_threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score(ct_preds, y_test, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7418218419728233"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_other_threshold = FixedThresholdClassifier(\n",
    "    grid_search.best_estimator_, threshold=0.81, response_method=\"predict_proba\"\n",
    ").fit(dembs_train, res_samp['toxic'])\n",
    "f1_score(test_samp['toxic'], classifier_other_threshold.predict(dembs_test))#,average=None)#82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dembs_train).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7115547284723397"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, classifier_other_threshold.predict(dembs_test))#82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_preds_adj = classifier_tuned.predict(dembs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(cl_preds_adj,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =pd.DataFrame({'a':grid_pred, 'b':test_sample['toxic']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'proba':list(probs[1])},\n",
    "              {'predicted':grid_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_res = pd.DataFrame(zip(y_test,probs[1],grid_pred),columns=['toxic', 'proba', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_res[(probs_res['toxic']!=probs_res['pred']&(probs_res['toxic']==1))]['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[1961]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.apply(lambda x: len(x))>2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.index.isin(res[res['a']!=res['b']].index))&(df['toxic']==1)]['text'].apply(lambda x: tokenizer.decode(chunker(x)))#.sample()\n",
    "\n",
    "\n",
    "\n",
    "#['b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(chunker(df.loc[128933]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[128933]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from phik.report import plot_correlation_matrix\n",
    "#from phik import phik_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phik_overview = phik_matrix(df[['toxic','word_count']],verbose=False)\n",
    "\n",
    "plot_correlation_matrix(\n",
    "    phik_overview.values,\n",
    "    x_labels=phik_overview.columns,\n",
    "    y_labels=phik_overview.index,\n",
    "    vmin=0, vmax=1, color_map='Blues',\n",
    "    title=r'correlation $\\phi_K$',\n",
    "    fontsize_factor=1.5,\n",
    "    figsize=(18, 14)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_lemm'], \n",
    "    df['toxic'], \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['toxic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_resample, y_resample = sampler.fit_resample(pd.DataFrame(X_train), pd.DataFrame(y_train)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resample.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 406,
    "start_time": "2025-08-18T19:09:24.283Z"
   },
   {
    "duration": 976,
    "start_time": "2025-08-18T19:09:47.447Z"
   },
   {
    "duration": 13,
    "start_time": "2025-08-18T19:09:57.004Z"
   },
   {
    "duration": 34,
    "start_time": "2025-08-18T19:11:01.787Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:11:51.371Z"
   },
   {
    "duration": 71,
    "start_time": "2025-08-18T19:18:11.221Z"
   },
   {
    "duration": 7324,
    "start_time": "2025-08-18T19:18:31.752Z"
   },
   {
    "duration": 2366,
    "start_time": "2025-08-18T19:18:42.522Z"
   },
   {
    "duration": 638,
    "start_time": "2025-08-18T19:18:46.740Z"
   },
   {
    "duration": 52,
    "start_time": "2025-08-18T19:19:05.932Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T19:20:05.266Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T19:21:09.819Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:21:24.068Z"
   },
   {
    "duration": 23,
    "start_time": "2025-08-18T19:22:05.917Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:23:17.506Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T19:24:15.955Z"
   },
   {
    "duration": 99,
    "start_time": "2025-08-18T19:24:30.236Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T19:24:50.180Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T19:25:21.649Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T19:25:35.470Z"
   },
   {
    "duration": 54,
    "start_time": "2025-08-18T19:27:41.878Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:27:55.628Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:28:25.512Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:28:36.733Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T19:43:50.418Z"
   },
   {
    "duration": 76,
    "start_time": "2025-08-18T19:43:53.290Z"
   },
   {
    "duration": 44,
    "start_time": "2025-08-18T19:43:58.942Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:01:42.440Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:04:30.308Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:08:23.893Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:08:51.300Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:09:05.354Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:09:48.303Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:10:01.688Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:11:33.427Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:11:38.262Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:11:39.629Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:11:50.462Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:11:51.853Z"
   },
   {
    "duration": 4391,
    "start_time": "2025-08-18T20:12:50.783Z"
   },
   {
    "duration": 4355,
    "start_time": "2025-08-18T20:13:13.006Z"
   },
   {
    "duration": 4625,
    "start_time": "2025-08-18T20:13:32.152Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:17:06.253Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T20:30:29.649Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:30:37.731Z"
   },
   {
    "duration": 3208,
    "start_time": "2025-08-18T20:30:40.953Z"
   },
   {
    "duration": 208,
    "start_time": "2025-08-18T20:31:13.163Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:31:43.147Z"
   },
   {
    "duration": 25,
    "start_time": "2025-08-18T20:32:15.848Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:32:37.920Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:32:47.420Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:35:57.628Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T20:38:26.028Z"
   },
   {
    "duration": 84,
    "start_time": "2025-08-18T20:41:50.277Z"
   },
   {
    "duration": 500,
    "start_time": "2025-08-18T20:42:03.816Z"
   },
   {
    "duration": 30,
    "start_time": "2025-08-18T20:42:27.568Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:42:43.879Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:42:52.612Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T20:43:10.466Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:43:34.481Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:45:39.302Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:46:42.581Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:46:46.700Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:47:07.407Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:48:23.658Z"
   },
   {
    "duration": 14,
    "start_time": "2025-08-18T20:48:42.988Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:49:12.478Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:49:21.311Z"
   },
   {
    "duration": 82,
    "start_time": "2025-08-18T20:50:04.814Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T20:50:41.375Z"
   },
   {
    "duration": 72,
    "start_time": "2025-08-18T20:50:44.499Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:51:17.776Z"
   },
   {
    "duration": 31,
    "start_time": "2025-08-18T20:51:32.035Z"
   },
   {
    "duration": 33,
    "start_time": "2025-08-18T20:52:03.189Z"
   },
   {
    "duration": 70,
    "start_time": "2025-08-18T20:53:02.892Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:53:17.975Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:53:35.266Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:53:46.379Z"
   },
   {
    "duration": 69,
    "start_time": "2025-08-18T20:53:47.797Z"
   },
   {
    "duration": 302,
    "start_time": "2025-08-18T20:54:14.768Z"
   },
   {
    "duration": 375,
    "start_time": "2025-08-18T20:54:27.606Z"
   },
   {
    "duration": 70,
    "start_time": "2025-08-18T20:54:33.372Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T20:54:52.299Z"
   },
   {
    "duration": 1172,
    "start_time": "2025-08-18T20:55:36.234Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:55:47.977Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:55:54.214Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:56:35.643Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T20:56:51.987Z"
   },
   {
    "duration": 320,
    "start_time": "2025-08-18T20:59:09.313Z"
   },
   {
    "duration": 8087,
    "start_time": "2025-08-18T20:59:46.419Z"
   },
   {
    "duration": 488,
    "start_time": "2025-08-18T21:00:10.574Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T21:00:32.098Z"
   },
   {
    "duration": 495,
    "start_time": "2025-08-18T21:00:33.903Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:00:36.626Z"
   },
   {
    "duration": 12,
    "start_time": "2025-08-18T21:00:37.887Z"
   },
   {
    "duration": 30,
    "start_time": "2025-08-18T21:00:41.452Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:00:57.292Z"
   },
   {
    "duration": 24,
    "start_time": "2025-08-18T21:01:12.068Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:07:00.983Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T21:07:27.166Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T21:07:28.929Z"
   },
   {
    "duration": 769,
    "start_time": "2025-08-18T21:07:48.911Z"
   },
   {
    "duration": 84,
    "start_time": "2025-08-18T21:08:43.198Z"
   },
   {
    "duration": 67,
    "start_time": "2025-08-18T21:11:21.466Z"
   },
   {
    "duration": 13733,
    "start_time": "2025-08-18T21:12:07.873Z"
   },
   {
    "duration": 357052,
    "start_time": "2025-08-18T21:12:26.388Z"
   },
   {
    "duration": 2420,
    "start_time": "2025-08-18T21:18:38.380Z"
   },
   {
    "duration": 1036,
    "start_time": "2025-08-18T21:18:40.802Z"
   },
   {
    "duration": 2780,
    "start_time": "2025-08-18T21:18:41.839Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:44.621Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T21:18:44.626Z"
   },
   {
    "duration": 873,
    "start_time": "2025-08-18T21:18:44.635Z"
   },
   {
    "duration": 16,
    "start_time": "2025-08-18T21:18:45.510Z"
   },
   {
    "duration": 50,
    "start_time": "2025-08-18T21:18:45.527Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:45.579Z"
   },
   {
    "duration": 4422,
    "start_time": "2025-08-18T21:18:45.584Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-18T21:18:50.007Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T21:18:50.015Z"
   },
   {
    "duration": 651,
    "start_time": "2025-08-18T21:18:50.028Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:50.680Z"
   },
   {
    "duration": 8,
    "start_time": "2025-08-18T21:18:50.684Z"
   },
   {
    "duration": 38,
    "start_time": "2025-08-18T21:18:50.694Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T21:18:50.734Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T21:18:50.741Z"
   },
   {
    "duration": 97352,
    "start_time": "2025-08-18T21:18:50.749Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.102Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.104Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.105Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.106Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.107Z"
   },
   {
    "duration": 55,
    "start_time": "2025-08-19T05:57:12.644Z"
   },
   {
    "duration": 1364,
    "start_time": "2025-08-19T05:57:18.742Z"
   },
   {
    "duration": 9004,
    "start_time": "2025-08-19T05:57:22.652Z"
   },
   {
    "duration": 25,
    "start_time": "2025-08-19T05:57:31.660Z"
   },
   {
    "duration": 2845,
    "start_time": "2025-08-19T05:57:59.992Z"
   },
   {
    "duration": 1205,
    "start_time": "2025-08-19T05:58:02.840Z"
   },
   {
    "duration": 3666,
    "start_time": "2025-08-19T05:58:04.048Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:07.716Z"
   },
   {
    "duration": 53,
    "start_time": "2025-08-19T05:58:07.721Z"
   },
   {
    "duration": 1056,
    "start_time": "2025-08-19T05:58:07.776Z"
   },
   {
    "duration": 28,
    "start_time": "2025-08-19T05:58:08.834Z"
   },
   {
    "duration": 79,
    "start_time": "2025-08-19T05:58:08.864Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-19T05:58:08.945Z"
   },
   {
    "duration": 4967,
    "start_time": "2025-08-19T05:58:08.951Z"
   },
   {
    "duration": 8,
    "start_time": "2025-08-19T05:58:13.919Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:13.929Z"
   },
   {
    "duration": 771,
    "start_time": "2025-08-19T05:58:13.934Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:14.708Z"
   },
   {
    "duration": 22,
    "start_time": "2025-08-19T05:58:14.713Z"
   },
   {
    "duration": 23175,
    "start_time": "2025-08-19T05:58:14.737Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.915Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.916Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.917Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.919Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.921Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.922Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T06:00:31.236Z"
   },
   {
    "duration": 2924,
    "start_time": "2025-08-19T06:01:01.130Z"
   },
   {
    "duration": 1170,
    "start_time": "2025-08-19T06:01:04.058Z"
   },
   {
    "duration": 3433,
    "start_time": "2025-08-19T06:01:05.230Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-19T06:01:08.665Z"
   },
   {
    "duration": 802,
    "start_time": "2025-08-19T06:01:08.672Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T06:01:09.476Z"
   },
   {
    "duration": 1034,
    "start_time": "2025-08-19T06:01:09.481Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-19T06:01:10.517Z"
   },
   {
    "duration": 56,
    "start_time": "2025-08-19T06:01:10.538Z"
   },
   {
    "duration": 69,
    "start_time": "2025-08-19T06:01:10.595Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-19T06:01:10.666Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-19T06:01:10.674Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-19T06:01:10.682Z"
   },
   {
    "duration": 3056180,
    "start_time": "2025-08-19T06:01:10.704Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-19T06:52:06.886Z"
   },
   {
    "duration": 37,
    "start_time": "2025-08-19T06:52:06.898Z"
   },
   {
    "duration": 345,
    "start_time": "2025-08-19T06:52:06.937Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.284Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.286Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.287Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.289Z"
   },
   {
    "duration": 26,
    "start_time": "2025-08-19T06:54:01.622Z"
   },
   {
    "duration": 98,
    "start_time": "2025-08-19T06:54:07.568Z"
   },
   {
    "duration": 273,
    "start_time": "2025-08-19T06:56:24.310Z"
   },
   {
    "duration": 47,
    "start_time": "2025-08-19T06:56:29.808Z"
   },
   {
    "duration": 26,
    "start_time": "2025-08-19T06:56:54.409Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-19T07:00:03.035Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-19T07:00:07.705Z"
   },
   {
    "duration": 43,
    "start_time": "2025-08-19T07:01:09.198Z"
   },
   {
    "duration": 22,
    "start_time": "2025-08-19T07:01:17.084Z"
   },
   {
    "duration": 14,
    "start_time": "2025-08-19T07:01:20.949Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
