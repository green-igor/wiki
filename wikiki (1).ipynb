{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn -q\n",
    "#!pip install wordcloud\n",
    "#!pip install swifter - q\n",
    "#!pip install xgboost\n",
    "#!pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import re\n",
    "from time import time \n",
    "import torch\n",
    "import swifter\n",
    "import transformers\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, make_scorer\n",
    "from __future__ import unicode_literals, print_function\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import FixedThresholdClassifier, TunedThresholdClassifierCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LeeCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LeeCo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://code.s3.yandex.net/datasets/toxic_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_count(dataframe, df_cleaning=False, stop_words_filtered = False):\n",
    "\n",
    "    if df_cleaning == True:\n",
    "        dataframe = dataframe.str.lower()\n",
    "        display('text lowered')\n",
    "        dataframe = dataframe.swifter.apply(lambda x: re.sub(r'\\W', ' ', x))\n",
    "        display('only words in text ')\n",
    "        dataframe = dataframe.swifter.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "        display('text is striped')\n",
    "        \n",
    "    \n",
    "    word2count = {}\n",
    "    \n",
    "    for data in dataframe:\n",
    "        words = nltk.word_tokenize(data)\n",
    "        for word in words:\n",
    "            if word not in word2count:\n",
    "                word2count[word] = 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "\n",
    "    if stop_words_filtered == True:\n",
    "        display('stop words are removed')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_word2count = {word: count for word, count in word2count.items() if word not in stop_words}\n",
    "        word_freq_df = pd.DataFrame(list(filtered_word2count.items()), columns=['Word', 'Frequency'])\n",
    "        word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "    else:\n",
    "        filtered_word2count = {word: count for word, count in word2count.items()}\n",
    "        word_freq_df = pd.DataFrame(list(filtered_word2count.items()), columns=['Word', 'Frequency'])\n",
    "        word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "    return word_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2716a17135ce494784e947d82975c6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['len'] = df['text'].swifter.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['len']>2500]['text'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(' '.join([clean_text(i) for i in nl_split(df['text'][3536])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(clean_text(df['text'][4581]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nl_split(df['text'][152428])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nl_split(df['text'][4581])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_rem(string_):\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(string_)\n",
    "    for url in urls:\n",
    "        string_ = re.sub(re.escape(url), ' xxlink ', string_)\n",
    "    \n",
    "    string_ = re.sub(r'https?://\\S+|www\\.\\S+', ' xxlink ', string_)\n",
    "    string_ = re.sub(r'<.*?>+', ' xxlink ', string_)\n",
    "    return string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(taken from:  xxlink  \\n\\nEuropean'"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " url_rem('(taken from: http://ec.europa.eu/justice_home/news/events/events_2008_en.html) \\n\\nEuropean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "ename": "PatternError",
     "evalue": "unbalanced parenthesis at position 63",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPatternError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[594], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m debug_samp \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m debug_samp\u001b[38;5;241m.\u001b[39mapply(clean_sents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[438], line 2\u001b[0m, in \u001b[0;36mclean_sents\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_sents\u001b[39m(string):\n\u001b[1;32m----> 2\u001b[0m     string \u001b[38;5;241m=\u001b[39m url_rem(string)\n\u001b[0;32m      3\u001b[0m     string \u001b[38;5;241m=\u001b[39m mail_user_rem(string)\n\u001b[0;32m      4\u001b[0m     sents \u001b[38;5;241m=\u001b[39m nl_split(string)\n",
      "Cell \u001b[1;32mIn[437], line 6\u001b[0m, in \u001b[0;36murl_rem\u001b[1;34m(string_)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(urls)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m----> 6\u001b[0m         string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n\u001b[0;32m      8\u001b[0m string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n\u001b[0;32m      9\u001b[0m string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<.*?>+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:208\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags, *args)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is passed as positional argument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    206\u001b[0m     )\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:350\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _compiler\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m DEBUG:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_compiler.py:748\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    747\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 748\u001b[0m     p \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse(p, flags)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:985\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbalanced parenthesis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgrouprefpos:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgroups:\n",
      "\u001b[1;31mPatternError\u001b[0m: unbalanced parenthesis at position 63"
     ]
    }
   ],
   "source": [
    "debug_samp = df['text'].sample(50)\n",
    "\n",
    "debug_samp.apply(clean_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97927                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [A couple of points, A. I don\"t think you\"re taking the Anonimu business too seriously, and I find Anittas\" intervention rather bizarre, but the key is to keep watching and waiting, and acting when really ready., It\"s frustrating that he can get away with so much (and his attacks have been pointed out to a number of administrators so far)., B. I too despise the profusion of flags, especially when coupled with recentism., Take for instance a biography I was looking at today: that of Anna Politkovskaya., It starts out nicely, but once she gets shot, spirals quickly out of control., Maybe, just maybe, the assassination deserves its own article, but as for the main article, a serious, scholarly biography would never look like this, especially the \"political reaction\" section., C. By the way, here are the other \"international reactions\" articles: 1, 2, 3, 4, 5, 6, 7., Some are just perhaps salvageable; with others, now that a few months have passed, \"who cares?\", seems a more fitting reaction. \"]\n",
       "104511                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Thanks I was unsure myself so I left them for some one else., I have an idea that is something hangs around CSD for more than 2 days it probably should go to AFD, because a lot of admins would have looked at it and done nothing!]\n",
       "149325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [Please read and notice the word jointly., (taken from: xxlink, European Hearing: \"Crimes Committed by Totalitarian Regimes\", On 8 April 2008 a European Hearing entitled \"crimes committed by totalitarian regimes\" took place in Brussels., This was organised jointly by the Slovenian Presidency of the European Union and the European Commission around the two key issues of recognition of the crimes and reconciliation. \"]\n",
       "45387                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [Stop, Stop Being Rude to users who don\"t sock this wiki.Thanks,yours truly,68.84.225.186]\n",
       "116154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Archiving, Please consider archiving this page., It\"s a pain to scroll all the way up and down.]\n",
       "40424                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [Welcome to Wikipedia., Vandalism is unnacceptable here., Please read the wikipedia rules before editing any articles further.]\n",
       "73282                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [The term has been used widely in Academic Journals too., See, for instance, The article that appeared in The Rutgers Journal of Law and Religion, xxlink, The term persecution appears in several places in the article., THe title says \" persecution of Falun Gong \"]\n",
       "157132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Please stop removing speedy deletion notices from pages that you have created yourself., If you continue, you will be blocked from editing Wikipedia., Talk]\n",
       "4997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Your happiness or unhappiness is not the primary goal here., Writing a reliable, trustworthy encyclopedia is., Currently, you\"re not assisting in that goal., In fact, you are inhibiting it.]\n",
       "24095                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [stifle, I hope you know I hate you., Kudos., dar dar dar]\n",
       "94167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [The True Founding of the Dominion of Canada, In 1862 the Duke of Newcastle (Henry Pelham Pelham Pelham-CLINTON) \"borrowed\" 12,000,000 pounds of silver from the PAPAL Bank of England to build useless PAPAL railroads as a \"treadmill of debt\" mechanism., The \"loan\" was to expire December 1867 so the need to refinance arose., The refinancing involved the concentration of the \"loans\" to; Nova Scotia, New Brunswick and Canada into a new central \"authority\" called the; \"Dominon of Canada\"., The precipitate, of course, involved the genocide of the Original, as opposed to the \"ab\"-original, people of Turtle Island., The whole process is centred upon the creation of so-called \"treaties\" then send your agents to anhilated the beneficiaries to the trust-treaty so the land would then \"naturally\" flow to the PAPACY as an escheat of the land to the vicar of Satan; POPE., If you would like to review the document concerning this banking fraud please feel free to contact me and I shall forward the same to you., Also, this is noted in the book; NEW WORLD ORDER, Old World Concentration found at; xxlink .\"]\n",
       "116763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [The page is a cfork (and also coatracking to boot)., I don\"t care under which title or transliteration the article resides, but clearly we do not need a Shahid article and a Istishhad one any more than we need a martyrdom article on top of martyr\"., One is the term for the act, the other the term for the person doing the act, but the topic is one and the same., (𒁳)]\n",
       "79385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [From my perspective my edit is 100% compatible with what is the Wiki policy for \"Reliable Sources\"., WP:No Original Research states: \"A primary source may only be used on Wikipedia to make straightforward, descriptive statements of facts that any educated person, with access to the source but without specialist knowledge, will be able to verify are supported by the source.\", My comments were entirely descriptive statements of fact that even a young school child would be able to verify as being true., There was no presence of analysis or interpretation in my statement., I merely said the website was shut down and a new message replaced its text regarding side effects., Do you have any additional feedback?]\n",
       "101600                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [A better intro, How about telling more about what the show is about?, Saying who owns it isn\"t that descriptive of an intro, and I actually got to this page wanting to know why these little people are fighting and such.]\n",
       "43819     [I am the author of Qutbi Bohra article., I had written this article just as an informative content for wikipedia and my intentions were to update the information regarding a new sect that was formed., I am a writer and have written many articles in different media., I love to write., I came to know of Wikipedia editing very recently through a friend when he showed me how wikipedia works and editors can make contributions., It was a time when i had heard of Qutbi Bohra and when i searched the internet i got evidence regarding the same in 3 newspapers mentioning it and a newsletter., This satisfied the noteablity ans verifyablity condition of the article., I gathered information regarding the sect from the community members whom i know and have recorded my discussion with them, with their permission., Further more i also got information regarding some past acts of Khuzaima Qutbuddin which were defamatory in nature but would prove the formation of the sect., But i did not use it as per the rules of wikipedia., I have written the entire article from third person point of view, as usually written for publications and have provided supportive evidence in most of the cases and due to frequent coercing by Ftutocdg i have also written a complete article,\"succession issue\" to explain the actual situation which Ftutocdg is trying to hide in his bid to get the Qutbi Bohra article removed., The reason that he is giving is just his point of view that he is trying to impose on the article., He has regularly done vandalism to this article and also has done the same to many Dawoodi Bohra articles along with some other editors in his group., As mentioned by Ftutocdg you can check the contribution i have made to Wikipedia, i have done all my edits with thorough study and reliable references., I would also like to suggest you to please look into the contribution of Ftutocdg and his group of editors they have just created a lot of chaos using the dark side of the very principles of Wikipedia the helps develop an article., I am sorry to say but i have spend more time giving explaination to Ftutocdg than write or edit an article., Whenever i login i first check whether he has vandalised the article or not., I have applied for protection in comments while reverting but i dont know the actual procedure involved., As regards your taggings i am very new to Wikipedia and i will require the help of experienced and well established administrators like you to guide me in my efforts., I request you if you can be more specific as to you tagging of the article in my talk page or article talk page so that i can take it up to the standards of Wikipedia., And also use the same principles when creating or editing other articles., Thank you again for all your efforts and time.]\n",
       "44950                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [get lost pinethicket, mind you own business and stop changing my edits I can edit whatever and whenever I want you don\"t like it say it to my face]\n",
       "136211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [i suggest you read links., Im on loads of celebrity sites!, Louise Glover is not a model, or half the people under GLAMOUR MODELS, and who are you?, this is a public domain!]\n",
       "45222                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Sorry, I couldn\"t quite remember precisely what you said, nor where you said it, so was careful not to directly attribute this to you., I understood you to be giving an opinion on the likely view taken by others, not that this was an instruction from you to me., But I remembered taking a very strict interpretation of what you said;, The strictest interpretation I regarded they would take was, \"a problem if you were editing and picking up new projects\"., I therefore felt inhibited from doing anything else., How does someone define \"ignored\"?, If I edited elsewhere for five minutes?, Five hours?, I would expect someone to put the boot in under almost any pretext - as in this indefinite block, whose reasoning was arrived at after the decision to block was made., Hence the \"review\" of my contributions after I took issue with his reasoning on his claims of breaches of policy. \"]\n",
       "101450                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [Emerald Witch?, Although \"Emerald Witch\" is listed as a name for her, she is more often called \"The Lady of the Green Kirtle\"., In fact, I can\"t find where in the text that she *is* called \"the Emerald Witch\"., Sources? \"]\n",
       "112429                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [work of fiction, this articel doesnt talk about the fiction put into the story., look at this link about the ss officer Goeth., xxlink, when you see stuff like this, it damages not only the credibility of the movie, but of the entire story.]\n",
       "105892                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [{| style=\"background-color:#F5FFFA; padding:0;\" cellpadding=\"0\", |style=\"border:1px solid #084080; background-color:#F5FFFA; vertical-align:top; color:#000000;\"|, Hello, !, Welcome to Wikipedia!, Thank you for your contributions to this free encyclopedia., If you decide that you need help, check out Getting Help below, ask me on my talk page, or place xxrbrct on your talk page and ask your question there., Please remember to sign your name on talk pages by clicking or using four tildes (~); this will automatically produce your name and the date., Finally, please do your best to always fill in the edit summary field., Below are some useful links to facilitate your involvement., Happy editing!, —EncMstr, {| width=\"100%\" style=\"background-color:#F5FFFA;\", |style=\"width: 55%; border:1px solid #FFFFFF; background-color:#F5FFFA; vertical-align:top\"|, Getting started Wikipedia tutorial • The five pillars of Wikipedia • Getting mentored, How to: edit a page • upload and use images Getting help Frequently asked questions • Tips, Where to ask questions or make comments, Request administrator attention Policies and guidelines Neutral point of view • No original research, Verifiability • Reliable sources • Citing sources, What Wikipedia is not • Biographies of living persons, Manual of Style • Three-revert rule • Sock puppetry, Copyrights • Fair use • Image use policy, External links • Spam • Vandalism, Deletion policy • Conflict of interest • Notability, |class=\"MainPageBG\" style=\"width: 55%; border:1px solid #FFFFFF; background-color:#F5FFFA; vertical-align:top\"|, {| width=\"100%\" cellpadding=\"0\" cellspacing=\"5\" style=\"vertical-align:top; background-color:#F5FFFA\", The community, |-, |style=\"color:#000\"|, Build consensus • Resolve disputes, Assume good faith • Civility • Etiquette, No personal attacks • No legal threats, [Wikipedia:Community Portal|Communit]\n",
       "136920                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [Same Topic, Another Request for Ban =, See Special:Contributions/GarGarOtts., Thanks., -Lyberty, =]\n",
       "119410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Seems to be resolved, for now anyway: the UMU website lists only Dr. Eastwood.]\n",
       "25131                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [McCain demanding that Wilson apologize, Please add a proper source to your recent edit., Thanks!, — tlk \"]\n",
       "6878                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [User page, Hey I\"m real sorry., I saw red link and assumed talk page, no offence was intended.]\n",
       "56399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [xxlink xxlink, = fake (and shit) official denial]\n",
       "7361                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [\" and in any case the \"physical impacts\" is not the place to talk about the initial cause\"]\n",
       "58458                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [With regards to your comments on Toraka: Please see Wikipedia\"s no personal attacks policy., \"Do not make personal attacks anywhere in Wikipedia., Comment on content, not on the contributor., Personal attacks damage the community and deter users.\", Please keep this in mind while editing., Thanks. \"]\n",
       "101592                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Invitation to a Wicnic in Gainesville on Saturday, June 22nd, Greetings!, Seeing that you\"re a member of WikiProject University of Florida, I\"m inviting to the North Central Florida 2013 Great American Wiknic that will be on Saturday June 22, 2013, commencing at 1:00 pm, ten blocks north of UF campus in Gainesville,., If you\"re able and inclined to come, please RSVP at at this URL., Type to you later,]\n",
       "57929                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [Actually, brings up a good point; to clarify:, No, the word \"cunt\" was not used as a personal attack against the blocking admin, but yes, it was used out of frustration to refer to •talk• \"]\n",
       "129876                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [supplied to Soviet army air force teams on the ground]\n",
       "143035                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [No, it does not., The \"world\"s third oldest airline\" refers to an airline that is still operating today., Have a look at the table again., Qantas is thirded by KLM and Avianca., However, according to the reference that is actually cited, Qantas claimed that they are the world\"s second oldest airline., xxlink \"]\n",
       "47809                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [It depends on what you mean by monsters., If you are talking about all of the monster that were sent and destroyed in a single episode that would probabally be considered too much trivia., If that is does it should only cover the main villain, the primary minions and only other monsters if they had a significant effect on the plot.76.66.180.54]\n",
       "107934                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [\"Regarding resolution 6:\", The Royal Naval decision was decreed on 1805-10-11; Trafalgar was 1805-10-21 and the new system was not then implemented there (look at the ships\" logs)., The wider Nautical Day was not directly affected thereby; one can hardly expect, for example, either the US Navy of the time or far-flung British merchant shipping to have considered Admiralty decisions (when eventually known) as binding., Don\"t use \"nautical\"., For the astronomical day, it would be well to say very explicitly at what instant of London civil time the change was made, or which 12 hours of the astronomical clock were jumped over., xxlink \"]\n",
       "156601                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Sory ToDST that was All My Roomates!, They came in and Stormed my computer and Called You Gay!, Which Your Not!]\n",
       "18860                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [Gordon Atkinson, Hi, thanks for your work on Atkinson., Unfortunately the article had been riddled with vandalism the day before by a random IP., As it wasn\"t picked up, your valid corrections were mixed up with bogus nonsense., I reverted the vandalism, but as I don\"t know the subject\"s life and work I can\"t properly re-add your work., I suspect that your impulse to edit the page came from seeing the vandalism in the first place., You could, perhaps, copy and paste your text back in with not so much difficulty as you know what\"s what., Thanks and best wishes \"]\n",
       "44703                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [\":The copyright-permitted obituary - permit sent Wikipedia - has now been included, translation approved by representative of copyright-holder., Eisfbnore: Your latest \"cleanup\" was a positive surprise, to the point that establishing final concensus now seems well possible (IMHO)., If you should wish to remove/change the now rpt obit-parts, pls do - left it in in deference to you., Perhaps link to original obit-text should be included xxlink ?, Apart fm these minor details, the entry now looks good - correct and informative - to my eyes., Hopefully 2u2.]\n",
       "65322                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [Have written a response to your comments on the use of the mile in Canada]\n",
       "44051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [\"=In response to your feedback=, You may change the information and please cite your sources., To learn how to cite sources, see Wikipedia:Citing sources and xxuser, talk]\n",
       "76584                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [=Strawpoll, Hello Bethling., First of all, the city of Boston has 559,034 as stated on their page., Since the city of Boston only contains this number, this is the population they have, and the population, based on the city defined boundaries does not include all of the metro area., To include all of the metro area would mean including other cities than Boston in the equation, when those numbers do not belong to Boston., I will fix Austin, I just caught the mistake., Thanks.]\n",
       "53351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [Thank you!, I\"ll make sure to let others know about it )]\n",
       "19104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [\":Calamitybrook, you might read some of the many, many responses made over time about these issues., You might consider wp:dispute resolution or wp:RfC to gather support for your position., However, your abusive behaviour is not likely to draw much in the way of help: see again, guidance on your talk page, this talk page, wp:COIN, wp:WQA., Please follow it.- Sinneed]\n",
       "17356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Yes it was., With two overlapping blocks, once the shorter one expires, the whole thing expires.]\n",
       "155942                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [And the name-checking of Led Zeppelin, The Who, Pink Floyd & The Rolling Stones looks like arbitrary aggrandisement as at the time they played at Leeds, Kershaw was 11!, He wouldn\"t be there for another 10 years!]\n",
       "13215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [\"= Your ignorant comments =, In response to your Please do not remove content from Wikipedia., It is considered vandalism., If you would like to experiment, please use the sandbox., Thank you., —khoi xxmail, Your record indicates that you were banned as a vandal several times and asked for a defense several times., Also, your record indicates that Bertil videt asked you to revert some legitimate changes without reason and you did it because he asked you, vandalazing good content that did not suit him or you., You should talk., Also please read your own talk page regarding many other warnings given to you by other users., Also be a man (at least try) and deal with your own talk page rather than begging others to hold your hand., Before acting as a functional illiterate, you should have read the pertinent prior discussion already took place in the talk page on the removed content which has no place in a biography., By the way, how is your boyfriend Bertil Videt doing?, I read sensational stuff on his talk page which he keeps hiding., Did you get to meet with his other boyfriends yet?]\n",
       "141898                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Speedy Deletion \"Dogsoc\" as patent nonsense, Hi Dreadstar, Could you please rehabilitate the \"Dogsoc\" article you deleted to some private talk space so that we can work with it and convert it to brilliant prose and all the other stuff., Would appreciate a little room / time to work on this one - since it is not patent nonsense although it may appear so at first blush. \"]\n",
       "155804                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Wow you play video games and you\"re nerdy. great time waste.;)]\n",
       "127623                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [Batta is used for kashmiri hindus, The non-muslims (hindus by default in modern sense) in kashmir valley especially brahmins were historically and still called \"Batta\" in kashmir valley as the word \"hindu\" was unknown to kashmiris., It is only in recent times that kashmiri batta (brahmins) have started calling themselves hindus something which they always detested., I never knew that Batta are also found in punjab and many khatris are derived from Batta caste, surprising and interesting.\"]\n",
       "11486                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [Why is there a disambiguation page for the Christian article then?, This is just more discrimination against christians.]\n",
       "148859                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [The Last Paragraph in the intro, I\"m not sure its point in being there., Could somebody put it in a more useful place- or at least explain it better?]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_samp.apply(clean_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please read and notice the word jointly.',\n",
       " '(taken from:  xxlink ',\n",
       " 'European Hearing: \"\"Crimes Committed by Totalitarian Regimes\"\"',\n",
       " 'On 8 April 2008 a European Hearing entitled \"\"crimes committed by totalitarian regimes\"\" took place in Brussels.',\n",
       " 'This was organised jointly by the Slovenian Presidency of the European Union and the European Commission around the two key issues of recognition of the crimes and reconciliation.   \"']"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[url_rem(sen) for sen in nl_split(df['text'][149325])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nPlease read and notice the word jointly. \\n\\n(taken from:  xxlink \\n\\nEuropean Hearing: \"\"Crimes Committed by Totalitarian Regimes\"\"\\nOn 8 April 2008 a European Hearing entitled \"\"crimes committed by totalitarian regimes\"\" took place in Brussels. This was organised jointly by the Slovenian Presidency of the European Union and the European Commission around the two key issues of recognition of the crimes and reconciliation.   \"'"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_rem(df['text'][149325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tp://ec.eu'"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][149325][60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mail_user_rem(string_):\n",
    "    \n",
    "    string_ = re.sub(r'[A-Za-z0-9\\_]*@[A-Za-z]*\\.?[A-Za-z0-9]*', ' xxmail ', string_)\n",
    "    string_ = re.sub(r'User:\\S+', ' xxuser ', string_)\n",
    "\n",
    "    return string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_split(string):\n",
    "    \n",
    "    string = re.split('\\n\\n|\\n',string)\n",
    "    result = [sent_tokenize(i.strip()) for i in string]\n",
    "    result = sum(result, [])\n",
    "    \n",
    "    return [i for i in result if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \n",
    "    string = re.sub(r'\\[{1,}.+\\]{1,}', ' xxbrct ', string)\n",
    "    string = re.sub(r'\\{{1,}.+\\}{1,}', ' xxrbrct ', string)\n",
    "    \n",
    "    string = re.sub(r'(\\W)\\1+|(\\D)\\1+', r'\\1', string)\n",
    "    string = re.sub(r'[\\_\\+]', ' ', string)\n",
    "    string = re.sub(\"['”‘’“]\", r'\"', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    \n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sents(string):\n",
    "    #string = url_rem(string)\n",
    "    sents = nl_split(string)\n",
    "    cl_sents = [url_rem(sent) for sent in sents]\n",
    "    cl_sents = [mail_user_rem(sent) for sent in cl_sents]\n",
    "    #string = mail_user_rem(string)\n",
    "    #sents = nl_split(string)\n",
    "    cl_sents = [clean_text(sent) for sent in cl_sents]\n",
    "    return cl_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean_sents(df['text'][80726])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(string):\n",
    "\n",
    "    sents_ = clean_sents(string)\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(sent, add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    #for sent in tokenized_sents:\n",
    "    #    if sent not in usents:# and sent!=[101,1012,102] and sent!=[101,102]:\n",
    "    #        usents.append(sent)\n",
    "\n",
    "    if len(tokenized_sents)>1:\n",
    "        result = sum(tokenized_sents,[])\n",
    "    else:\n",
    "        result = tokenized_sents[0]\n",
    "    \n",
    "        \n",
    "    if len(result)>512:\n",
    "\n",
    "        result = result[:129] + result[-382:]+[102]\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 4717,\n",
       " 19888,\n",
       " 2080,\n",
       " 1010,\n",
       " 11320,\n",
       " 6873,\n",
       " 1010,\n",
       " 1004,\n",
       " 4040,\n",
       " 2666,\n",
       " 3104,\n",
       " 2005,\n",
       " 13157,\n",
       " 2213,\n",
       " 1004,\n",
       " 12403,\n",
       " 2213,\n",
       " 2037,\n",
       " 2743]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker(df['text'][80726])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PatternError",
     "evalue": "unbalanced parenthesis at position 30",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPatternError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[483], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(chunker)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[322], line 3\u001b[0m, in \u001b[0;36mchunker\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunker\u001b[39m(string):\n\u001b[1;32m----> 3\u001b[0m     sents_ \u001b[38;5;241m=\u001b[39m clean_sents(string)\n\u001b[0;32m      5\u001b[0m     tokenized_sents \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(sent, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents_]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#for sent in tokenized_sents:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#    if sent not in usents:# and sent!=[101,1012,102] and sent!=[101,102]:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#        usents.append(sent)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[438], line 2\u001b[0m, in \u001b[0;36mclean_sents\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_sents\u001b[39m(string):\n\u001b[1;32m----> 2\u001b[0m     string \u001b[38;5;241m=\u001b[39m url_rem(string)\n\u001b[0;32m      3\u001b[0m     string \u001b[38;5;241m=\u001b[39m mail_user_rem(string)\n\u001b[0;32m      4\u001b[0m     sents \u001b[38;5;241m=\u001b[39m nl_split(string)\n",
      "Cell \u001b[1;32mIn[437], line 6\u001b[0m, in \u001b[0;36murl_rem\u001b[1;34m(string_)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(urls)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m----> 6\u001b[0m         string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n\u001b[0;32m      8\u001b[0m string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n\u001b[0;32m      9\u001b[0m string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<.*?>+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:208\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags, *args)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is passed as positional argument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    206\u001b[0m     )\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:350\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _compiler\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m&\u001b[39m DEBUG:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_compiler.py:748\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    747\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 748\u001b[0m     p \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse(p, flags)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:985\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbalanced parenthesis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgrouprefpos:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mgroups:\n",
      "\u001b[1;31mPatternError\u001b[0m: unbalanced parenthesis at position 30"
     ]
    }
   ],
   "source": [
    "df['text'].sample(50).apply(chunker).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([101, 7526, 102, 101, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310, 18442, 13076, 12392, 2050, 5470, 2020, 16407, 1029, 102, 101, 2027, 4694, 1000, 1056, 3158, 9305, 22556, 1010, 2074, 8503, 2006, 2070, 3806, 2044, 1045, 5444, 2012, 2047, 2259, 14421, 6904, 2278, 1012, 102, 101, 1998, 3531, 2123, 1000, 1056, 6366, 1996, 23561, 2013, 1996, 2831, 3931, 2144, 1045, 1000, 1049, 3394, 2085, 1012, 6486, 1012, 16327, 1012, 4229, 1012, 2676, 102]),\n",
       "       list([101, 1040, 1000, 22091, 2860, 999, 102, 101, 2002, 3503, 2023, 4281, 6120, 1045, 1000, 1049, 9428, 5881, 2007, 1012, 102, 101, 4283, 1012, 102, 101, 1006, 2831, 1007, 2538, 1024, 4868, 1010, 2254, 2340, 1010, 2355, 1006, 11396, 1007, 102]),\n",
       "       list([101, 4931, 2158, 1010, 1045, 1000, 1049, 2428, 2025, 2667, 2000, 10086, 2162, 1012, 102, 101, 2009, 1000, 1055, 2074, 2008, 2023, 3124, 2003, 7887, 9268, 7882, 2592, 1998, 3331, 2000, 2033, 2083, 10086, 2015, 2612, 1997, 2026, 2831, 3931, 1012, 102, 101, 2002, 3849, 2000, 2729, 2062, 2055, 1996, 4289, 3436, 2084, 1996, 5025, 18558, 1012, 102]),\n",
       "       list([101, 2062, 102, 101, 1045, 2064, 1000, 1056, 2191, 2151, 2613, 15690, 2006, 7620, 1011, 1045, 4999, 2065, 1996, 2930, 6747, 2323, 2022, 2101, 2006, 1010, 2030, 1037, 4942, 29015, 1997, 1000, 4127, 1997, 13436, 1000, 1011, 1045, 2228, 1996, 7604, 2089, 2342, 29369, 2075, 2061, 2008, 2027, 2024, 2035, 1999, 1996, 6635, 2168, 4289, 29464, 3058, 4289, 4385, 1012, 102, 101, 1045, 2064, 2079, 2008, 2101, 2006, 1010, 2065, 2053, 1011, 2028, 2842, 2515, 2034, 1011, 2065, 2017, 2031, 2151, 18394, 2005, 4289, 3436, 2806, 2006, 7604, 2030, 2215, 2000, 2079, 2009, 4426, 3531, 2292, 2033, 2113, 1012, 102, 101, 2045, 3544, 2000, 2022, 1037, 2067, 21197, 2006, 4790, 2005, 3319, 2061, 1045, 3984, 2045, 2089, 2022, 1037, 8536, 2127, 1037, 12027, 4332, 2039, 1012, 102, 101, 2009, 1000, 1055, 3205, 1999, 1996, 7882, 2433, 1041, 2290, 16948, 1024, 2204, 3720, 9930, 1001, 3665, 1000, 102]),\n",
       "       list([101, 2017, 1010, 2909, 1010, 2024, 2026, 5394, 1012, 102, 101, 2151, 3382, 2017, 3342, 2054, 3931, 2008, 1000, 1055, 2006, 1029, 102])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][:5].apply(lambda x: chunker(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\n Matt Crypto, Lupo, & Stormie cover for Nazism & spam their rants http://rexcurry.net/wikipedialies.html \\n\\nThere are some regular writers on Wikipedia who knowingly engage in personal attacks and deletions (even of links) of anyone who explains the connection between Edward Bellamy and his cousin Francis Bellamy (author of the pledge of allegiance) and National Socialism. Some of the worst falsifiers on Wikipedia are Stormie, Lupo, and Matt Crypto. They were challenged to respond to the facts that they suppressed or to concede, and they each conceded that the facts that they suppress are correct and that they delete those facts because they do not want Wikipedia readers to know the truth. They constantly spam their own point of view in their diatribes in which they try to cover up for Nazism. It is behavior that breaks Wikipedia\\'s rules. What kind of people cover-up for Nazis and that horrid ideology?\\n\\nMany regular writers on Wikipedia are intellectually dishonest  and they regurgitate the top media cover-ups.  They cover up many historic photos of the Pledge of Allegiance http://rexcurry.net/pledge2.html that reveal its original straight-arm salute.  They cover up the fact that the author of the Pledge of Allegiance (Francis Bellamy), and his cousin and cohort (Edward Bellamy), were National Socialists and supported the \"\"Nationalism\"\" movement, the \"\"Nationalist\"\" magazine, the \"\"Nationalist Educational Association,\"\" their dogma of \"\"military socialism,\"\" and Edward inspired the \"\"Nationalist Party.\"\"  They cover up the Pledge\\'s straight-arm salute as the origin http://rexcurry.net/pledge1.html of the salute of the National Socialist German Workers\\' Party (Nazis). As part of the cover up, they perpetuate the myth http://rexcurry.net/pledgesalute.html that the straight-arm salute was from ancient Rome.  They cover up discoveries by the historian Rex Curry.  The Pledge began with a military salute that then stretched outward toward the flag. Historic photographs are at http://rexcurry.net/pledge2.html and at http://rexcurry.net/pledge_military.html showing the evolution of the gesture. Due to the way that both gestures were used, the military salute led to the Nazi salute. The Nazi salute http://rexcurry.net/book1a1contents-pledge.html is an extended military salute. A mnemonic device is the swastika (\"\"Hakenkreuz\"\" in German).   Although the swastika was an ancient symbol, it was also used sometimes by German National Socialists to represent \"\"S\"\" letters for their \"\"socialism.\"\"  Hitler altered his own signature http://rexcurry.net/book1a1contents-swastika.html to use the same stylized \"\"S\"\" letter for \"\"socialist.\"\" Wikipedia\\'s fibbers cover up for the swastika http://rexcurry.net/swastikanews.html and its use as a sick socialist symbol. Wikipedia\\'s dissemblers cover up for the National Socialist German Worker\\'s Party http://rexcurry.net/swastikamedia.html by overuse of the hackneyed shorthand \"\"Nazi.\"\"  Many Wikipedia posters are propagandists in that there was no \"\"Nazi Party\"\" because it was actually the \"\"National Socialist German Workers\\' Party\"\" and the party members did not call themselves \"\"Nazis\"\" nor the \"\"Nazi Party.\"\"  The term \"\"Nazi\"\" developed from slang using the first syllable in the German pronunciation of the \"\"National Socialist German Workers\\' Party.\"\"  In that sense, the author of the Pledge of Allegiance was a \"\"Nazi\"\" too, in that the term means \"\"National Socialist.\"\"  The term \"\"Nazi\"\" is also used to hide the National Socialist dogma behind the Pledge of Allegiance, its original gesture, and the National Socialist German Workers\\' Party.  \\n\\nWikipedia\\'s cons use the term \"\"Nazi\"\" and the misnomer \"\"Nazi Party\"\" to cover-up the horrors of socialism. Wikipedia\\'s falsifiers cover up the the socialist trio of atrocities and and their socialist Wholecaust (of which the Holocaust was a part): 62 million people were slaughtered under the former Union of Soviet Socialist Republics; 35 million were slaughtered under the Peoples\\' Republic of China; 21 million were slaughtered under the National Socialist German Workers\\' Party. They cover up the fact that socialists helped start WWII with the National Socialist German Workers\\' Party and the Union of Soviet Socialist Republics together  http://rexcurry.net/socialistwar.html as allies in 1939. They cover up for and are deniers of the socialist Wholecaust, http://rexcurry.net/mediacoverup.html of which the monstrous Holocaust was a part.\\n\\nLiars on Wikipedia repeat common lies http://rexcurry.net/mediapledge.html of the media. http://rexcurry.net/socialistmedia.html\\n\\nWikipedia should not be cited for support because it is no different than quoting various anonymous sources who have no knowledge of the topic http://rexcurry.net/wikipedialies.html or who have fibs to spread about the topic.\\n\\nThe Wikipedia article for the \"\"Roman salute\"\" used to be complet'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][80726]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toxic = df[df['toxic']==1]#['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_toxic = df[df['toxic']==0]#['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_w_trash = w_count(\n",
    "    df['text'],\n",
    "    df_cleaning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_w_trash0 = w_count(\n",
    "    df_non_toxic['text'],\n",
    "    df_cleaning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_w_trash1 = w_count(\n",
    "    df_toxic['text'],\n",
    "    df_cleaning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddac81a8ddbf45ee83170fa48520fd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a17dfb28c4f328f53b29a88856498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo_trash = w_count(\n",
    "    df['text'],\n",
    "    df_cleaning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36f0891af3a4610bffb10bf70a5eb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a7ff448bcb4bf4b1307d7a62ac027d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo_trash0 = w_count(\n",
    "    df_non_toxic['text'],\n",
    "    df_cleaning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30b5ec8281542e48e8d9adc20d2cf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1737e16178904d84a61211575eff82c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo_trash1 = w_count(\n",
    "    df_toxic['text'],\n",
    "    df_cleaning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c246aa50ff49ff8ccfaee47c8bda8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ce35b28b2a43ef83124fc4369db7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stop words are removed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo = w_count(\n",
    "    df['text'],\n",
    "    df_cleaning=True,\n",
    "    stop_words_filtered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b60b24932294346b76d13169b3c9b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c6786edc964857b6836a6a66aa21d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stop words are removed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo0 = w_count(\n",
    "    df_non_toxic['text'],\n",
    "    df_cleaning=True,\n",
    "    stop_words_filtered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text lowered'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b50430512744dc39fd726aa269f52da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'only words in text '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729346d32e5041f49607a2a95c7f5fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text is striped'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stop words are removed'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_wo1 = w_count(\n",
    "    df_non_toxic['text'],\n",
    "    df_cleaning=True,\n",
    "    stop_words_filtered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bars(df_list):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3,figsize=(12,8))\n",
    "    \n",
    "    ax[0].barh(df_list[0]['Word'],df_list[0]['Frequency'])\n",
    "    ax[1].barh(df_list[1]['Word'],df_list[1]['Frequency'])\n",
    "    ax[2].barh(df_list[2]['Word'],df_list[2]['Frequency'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bars([words_w_trash.head(50),words_w_trash0.head(50),words_w_trash1.head(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bars([words_wo_trash.head(50),words_wo_trash0.head(50),words_wo_trash1.head(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bars([words_wo.head(50),words_wo0.head(50),words_wo1.head(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for item in [words_wo_trash,words_wo_trash0,words_wo_trash1]:\n",
    "    display(item[item['Frequency']>1].tail(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for item in [words_w_trash,words_w_trash0,words_w_trash1]:\n",
    "    display(item[item['Frequency']>1].tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_split(string):\n",
    "    \n",
    "    string = re.split('\\n+',string)\n",
    "    result = [sent_tokenize(i.strip()) for i in string]\n",
    "        \n",
    "    return sum(result,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e4aab233514f7a827a8653e205c984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\swifter\\swifter.py:311\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[1;32m--> 311\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;124;03mGiven a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03mGiven a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03mtesting candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03mTrue, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03mfollows the period.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1328\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1328\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1457\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1456\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1457\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_pair_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrealign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1429\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1428\u001b[0m last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1429\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_contains_sentbreak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1394\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Series'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswifter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\swifter\\swifter.py:329\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_apply(func, convert_dtype, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# use pandas\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pandas_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\swifter\\swifter.py:235\u001b[0m, in \u001b[0;36mSeriesAccessor._pandas_apply\u001b[1;34m(self, df, func, convert_dtype, *args, **kwds)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar:\n\u001b[0;32m    234\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar_desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas Apply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mapply(func, convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:144\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['word_count'] = df['text'].swifter.apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sent_count'] = df['text'].swifter.apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['len'] = df['text'].swifter.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==1]['word_count'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==0]['word_count'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==1]['sent_count'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==0]['sent_count'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==1]['len'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['toxic']==0]['len'].hist(bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plt(df_list, name, val_list, plt_xlim=1):\n",
    "\n",
    "    x_max = min([series.max() for series in df_list])* plt_xlim\n",
    "\n",
    "    fig, ax = plt.subplots(2,1,figsize=(10,8))\n",
    "    ax[0].boxplot([df for df in df_list], vert=False)\n",
    "\n",
    "    ax[0].set_yticklabels(val_list)\n",
    "    ax[0].set_xlabel(name)\n",
    "    ax[0].set_xlim([0, x_max])\n",
    "\n",
    "    ax[1].hist([df for df in df_list], histtype='stepfilled', bins=500)\n",
    "    ax[1].set_ylabel('частота')\n",
    "    ax[1].set_xlabel(name)\n",
    "    ax[1].set_xlim([0, x_max])\n",
    "\n",
    "    \n",
    "    plt.suptitle(f'Ящики с усами и гистограмма для признака \"{name}\"')\n",
    "    plt.legend(val_list);\n",
    "    #plt.xlim((0,x_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {'word_count' : 'количество слов',\n",
    " 'sent_count' : 'количество предложений',\n",
    " 'len' : 'количество символов'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in ['word_count', 'sent_count', 'len']:\n",
    "    \n",
    "    distribution_plt(\n",
    "        \n",
    "        [df[df['toxic']==1][col], df[df['toxic']==0][col]],\n",
    "        \n",
    "        name_dict[col],\n",
    "        \n",
    "        ['токсичные\\nкомментарии', 'не токсичные\\nкомментарии'],\n",
    "        \n",
    "        0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102, '[MASK]': 103}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \n",
    "    \n",
    "    string = url_extr(string)\n",
    "\n",
    "    #if len(urls)!=0:\n",
    "    #    for url in urls:\n",
    "    #        string = re.sub(url, ' xxlink ', string)\n",
    "\n",
    "    \n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', ' xxlink ', string) #' [lnkt] '\n",
    "    string = re.sub(r'<.*?>+', ' xxlink ', string)\n",
    "    string = re.sub(r'[A-Za-z0-9\\_]*@[A-Za-z]*\\.?[A-Za-z0-9]*', ' xxmail ', string)\n",
    "\n",
    "    string = re.sub(r'User:\\S+', ' xxuser ', string)\n",
    "    \n",
    "    string = re.sub(r'\\[{1,2}.+\\]{1,2}', ' xxbrct ', string)\n",
    "    string = re.sub(r'\\{{1,2}.+\\}{1,2}', ' xxrbrct ', string)\n",
    "    #string = re.sub('\\\\d+\\\\.?', ' dgtl ', string)\n",
    "    string = re.sub(r'(\\W)\\1+|(\\D)\\1+', r'\\1', string)\n",
    "    string = re.sub(r'[\\_\\+]',' ',string)\n",
    "    string =  re.sub(\"['”‘’“]\", r'\"', string)\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    \n",
    "    return string.strip()#.lower() #best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean_text(string):\n",
    "\n",
    "    string = string.strip()\n",
    "    #string = re.sub(r'\\[?https?://\\S+\\]?|\\[?www\\.\\S+\\]?', ' [lnkt] ', string)\n",
    "\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', ' lnkt ', string)\n",
    "    \n",
    "    string = re.sub(r'\\[{1,}.+\\]{1,2}', ' brct ', string)\n",
    "    string = re.sub(r'\\{{1,}.+\\}{1,}', ' rbrct ', string)\n",
    "    #string = re.sub('\\\\d+\\\\.?', ' dgtl ', string)\n",
    "    string = re.sub(r'(\\W)\\1+|(\\D)\\1+', r'\\1', string)\n",
    "    string = re.sub(r'[\\_\\+\\-]',' ',string)\n",
    "    \n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    \n",
    "    return string.strip()#.lower() #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def chunker(string):\n",
    "\n",
    "    sents_ = nl_split(string)\n",
    "\n",
    "    #tokenized_sents = [tokenizer.encode(clean_text(sent), add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(clean_text(sent), add_special_tokens=True) for sent in sents_]\n",
    "\n",
    "    usents=[]\n",
    "    usents=tokenized_sents\n",
    "    \n",
    "    #for sent in tokenized_sents:\n",
    "    #    if sent not in usents:# and sent!=[101,1012,102] and sent!=[101,102]:\n",
    "    #        usents.append(sent)\n",
    "    \n",
    "    if len(usents)>1:\n",
    "       result =  sum(usents, [])\n",
    "    \n",
    "    elif len(usents)==1:\n",
    "        result = usents[0]\n",
    "    \n",
    "    else:\n",
    "        result = tokenizer.encode(sents_[0])\n",
    "\n",
    "    if len(set(result))/len(string)<0.02:\n",
    "        result = list(dict.fromkeys(result))\n",
    "        \n",
    "    #if len(result)>512:\n",
    "    #    result = result[:511]+[102]\n",
    "    #    result = [101] + result[:511]+[102]\n",
    "        #result = result[:129] + result[-382:]+[102]\n",
    "        \n",
    "    \n",
    "    return [101] + result[:510]+[102]#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(string):\n",
    "\n",
    "    sents_ = nl_split(string)\n",
    "\n",
    "    sents_clean = [clean_text(sent) for sent in  sents_]\n",
    "\n",
    "    tokenized_sents = [tokenizer.encode(sent, add_special_tokens=True) for sent in sents_clean]\n",
    "\n",
    "    #for sent in tokenized_sents:\n",
    "    #    if sent not in usents:# and sent!=[101,1012,102] and sent!=[101,102]:\n",
    "    #        usents.append(sent)\n",
    "\n",
    "    if len(tokenized_sents)>1:\n",
    "        result = sum(tokenized_sents,[])\n",
    "    else:\n",
    "        result = tokenized_sents[0]\n",
    "    \n",
    "        \n",
    "    if len(result)>512:\n",
    "\n",
    "        result = result[:129] + result[-382:]+[102]\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[250], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chunker(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msample())\n",
      "Cell \u001b[1;32mIn[249], line 5\u001b[0m, in \u001b[0;36mchunker\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunker\u001b[39m(string):\n\u001b[0;32m      3\u001b[0m     sents_ \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mapply(nl_split)\n\u001b[1;32m----> 5\u001b[0m     sents_clean \u001b[38;5;241m=\u001b[39m [clean_text(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m  sents_\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m      7\u001b[0m     tokenized_sents \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(sent, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents_clean]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#for sent in tokenized_sents:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#    if sent not in usents:# and sent!=[101,1012,102] and sent!=[101,102]:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#        usents.append(sent)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[244], line 4\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_text\u001b[39m(string):\n\u001b[1;32m----> 4\u001b[0m     string \u001b[38;5;241m=\u001b[39m url_extr(string)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#if len(urls)!=0:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#    for url in urls:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#        string = re.sub(url, ' xxlink ', string)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     string \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string) \u001b[38;5;66;03m#' [lnkt] '\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[241], line 3\u001b[0m, in \u001b[0;36murl_extr\u001b[1;34m(string_)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21murl_extr\u001b[39m(string_):\n\u001b[0;32m      2\u001b[0m     extractor \u001b[38;5;241m=\u001b[39m URLExtract()\n\u001b[1;32m----> 3\u001b[0m     urls \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mfind_urls(string_)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m      5\u001b[0m         string_ \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m xxlink \u001b[39m\u001b[38;5;124m'\u001b[39m, string_)    \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urlextract\\urlextract_core.py:958\u001b[0m, in \u001b[0;36mURLExtract.find_urls\u001b[1;34m(self, text, only_unique, check_dns, get_indices, with_schema_only)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(urls)\n\u001b[0;32m    957\u001b[0m result_urls: List[Union[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 958\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(urls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    959\u001b[0m url_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m url:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urlextract\\urlextract_core.py:876\u001b[0m, in \u001b[0;36mURLExtract.gen_urls\u001b[1;34m(self, text, check_dns, get_indices, with_schema_only)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03mCreates generator over found URLs in given text.\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m:rtype: str|tuple(str, tuple(int, int))\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    875\u001b[0m tld_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 876\u001b[0m matched_tlds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tlds_re\u001b[38;5;241m.\u001b[39mfindall(text)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m matched_tlds:\n\u001b[0;32m    879\u001b[0m     tld \u001b[38;5;241m=\u001b[39m matched_tlds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "chunker(df['text'].sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenizer.decode(chunker(df['text'][96905]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenizer.decode(chunker(df['text'][4704]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker(df['text'][82496]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['first_sent'] = df['text'].swifter.apply(lambda x: nl_split(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df['after_tok'] = df['text'].swifter.apply(lambda x: tokenizer.decode(chunker(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['a_t_len'] = df['after_tok'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['after_tok'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orientation_list = ['gay', 'lesb', 'trans', 'lgbt']\n",
    "relig_list = ['christ', 'islam', 'judai','budd', 'jesus', 'allah', 'yahweh']\n",
    "family_list = ['mom', 'dad', 'papa', 'daddy', 'son', 'daughter', 'mother', 'child']\n",
    "\n",
    "df['orient'] =  df['text'].str.contains('|'.join(orientation_list), flags=re.IGNORECASE)\n",
    "df['relig'] =  df['text'].str.contains('|'.join(relig_list), flags=re.IGNORECASE)\n",
    "df['family'] =  df['text'].str.contains('|'.join(family_list), flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "df.sort_values(by='after_tok').iloc[50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.drop_duplicates(subset='after_tok', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logy(feats, target):\n",
    "    lr = LogisticRegression(random_state=RANDOM_STATE,max_iter=100000,class_weight='balanced')\n",
    "    lr.fit(feats, target)\n",
    "    scores = cross_val_score(lr, feats, target, cv=5, scoring='f1_macro')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3185*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(n=32000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31840.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "159200*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #samp[['text','orient','relig','family']],\n",
    "    samp['text'],\n",
    "    samp['toxic'],\n",
    "    test_size = 0.5,\n",
    "    random_state = RANDOM_STATE,\n",
    "    stratify = samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000,), (16000,), (16000,), (16000,))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_resample, y_resample = sampler.fit_resample(pd.DataFrame(X_train), pd.DataFrame(y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_resample.shape, y_resample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res_df = X_resample.join(y_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res_samp = res_df.sample(n=25000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_samp = pd.DataFrame(X_test).join(pd.DataFrame(y_test)).sample(n=res_samp.shape[0], random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res_samp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_samp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(self, text, batch_size=1):\n",
    "        \n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        #self.model_class ,self.pretrained_weights = (\n",
    "        #    transformers.BertModel, 'bert-base-uncased')\n",
    "        self.model_class ,self.pretrained_weights = (\n",
    "            transformers.DistilBertModel, 'distilbert-base-uncased')    \n",
    "        \n",
    "        self.model = self.model_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "    \n",
    "    def get(self):\n",
    "        \n",
    "        tokenized = self.text.swifter.apply(lambda x: chunker(x))\n",
    "        \n",
    "        max_len = 0\n",
    "        \n",
    "        for tok in tokenized.values:\n",
    "            if len(tok) > max_len:\n",
    "                max_len = len(tok)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        \n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        for i in notebook.tqdm(range(len(padded) // batch_size)):\n",
    "                batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "                attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.model(batch, attention_mask=attention_mask_batch)\n",
    "                \n",
    "                embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "    \n",
    "        features = np.concatenate(embeddings)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[df['text'].str.contains('I ass. I ass.')]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker(df['text'][4703]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker(df['text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.decode(chunker('someone needs to remove lott 71. 227. 167. 147'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2f5760cd03435991ec8fa2cac666e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75876d74876a41b7b91c06ef94094d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.11305679, -0.02071994,  0.11411636, ...,  0.08299962,\n",
       "         0.27643913,  0.33111763],\n",
       "       [-0.13494527, -0.0798372 ,  0.2560554 , ..., -0.03249058,\n",
       "         0.27541596,  0.4066586 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertTokenizer(df['text'][0:2]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16000/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't vandalize other wikipedians pages. 2nd warning. link\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['text'][5327])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa680f4aa65b4aa6b71aaab1880f2ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf990e8384e435bb19cf93ebf322c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dembs_train = BertTokenizer(text=X_train, batch_size=32).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8099593303107095)"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logy(dembs_train, y_train.values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8073131369641621"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logy(dembs_train, y_train.values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3227ffdf19714ccca59d93912848c410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6692edc8254eb9b5c3dc826c34df6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dembs_test = BertTokenizer(text=X_test, batch_size=32).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#mod = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embs(string, model ):\n",
    "    sents_ = nl_split(string)\n",
    "\n",
    "    sents_ = [clean_text(sent) for sent in sents_]\n",
    "    \n",
    "    tokens = tokenizer(sents_,max_length=512, padding=True, truncation=True, return_tensors='np')\n",
    "    #model = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(torch.LongTensor(tokens['input_ids']), attention_mask=torch.LongTensor(tokens['attention_mask']))\n",
    "\n",
    "    embs = batch_embeddings[0][:,0,:].numpy()\n",
    "    \n",
    "    \n",
    "    return np.mean(embs, axis=0)#.reshape(1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dembs_train).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8030408159999999"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.80239492, 0.7986666 , 0.8033005 , 0.79858842, 0.81225364]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([0.8919973 , 0.8999975 , 0.90249588, 0.89397286, 0.89449786])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###dembs_test = BertTokenizer(text=test_samp['text'], batch_size=50).get()\n",
    "####logy(dembs_train, res_samp['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dembs_test = BertTokenizer(text=X_test['text'], batch_size=50).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr = pd.DataFrame(trial).join(pd.DataFrame(res_samp['toxic'].reset_index(drop=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tr.head().iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr.tail().iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tr.drop_duplicates(inplace=True)\n",
    "#tr.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr.tail().iloc[:,-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train \n",
    ")\n",
    "\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y_true, y_pred,):\n",
    "\n",
    "    f1 = f1_score(y_pred, y_true, average=None)[1]\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_xgb(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    f1 = f1_score( y_pred,y_true, average=None)[1]\n",
    "\n",
    "    return 'f1', f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "my_func = make_scorer(f1_eval, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('models',[LogisticRegression(random_state=RANDOM_STATE,class_weight='balanced')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wei = y_train.value_counts()[0]/y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__solver': ['newton-cg', 'liblinear'],\n",
    "        #'models__class_weight':,\n",
    "        #'models__C': [0.5,1,3,5],#,\n",
    "        #'models__penalty': ['l1', 'l2']\n",
    "          \n",
    "    },\n",
    "\n",
    "    {\n",
    "        'models': [XGBClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            ###objective = 'binary:logistic',\n",
    "            eval_metric = f1_xgb,\n",
    "            #maximize=True,\n",
    "            #eval_metric='logloss',\n",
    "            #scale_pos_weight = y_train.value_counts(normalize=1)[0]\n",
    "            scale_pos_weight = p_wei\n",
    "            \n",
    "            )],\n",
    "        'models__n_estimators':[500, 1000, 1250],#[1000, 1250],#, 1500, 2000],#[1000, 1250, 1500, 2000],\n",
    "        'models__max_depth':[4],#[4,5,6],\n",
    "        'models__learning_rate':[0.3,0.2, 0.1, 0.4]#[0.1,0.2, 0.25 , 0.3],[0.3,0.4,0.2, 0.1, 0.05]\n",
    "        #'models__alpha':[0.1, 0.3]#,\n",
    "        #'models__eval_metric': [f1_xgb]\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    0.896813\n",
       "1    0.103188\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.691096305269534)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()[0]/y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__solver': ['newton-cg', 'liblinear'],\n",
    "        #'models__class_weight':,\n",
    "        'models__C': [0.5,1,3,5],\n",
    "        #'models__penalty': ['l1', 'l2']\n",
    "          \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;models&#x27;,\n",
       "                                        [LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                            random_state=88)])]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;models&#x27;: [LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                        random_state=88)],\n",
       "                          &#x27;models__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;liblinear&#x27;]},\n",
       "                         {&#x27;models&#x27;: [XGBClassifier(base_score=None,\n",
       "                                                   booster=None, callbacks=None,\n",
       "                                                   colsample_bylevel=None,\n",
       "                                                   c...\n",
       "                                                   max_delta_step=None,\n",
       "                                                   max_depth=None,\n",
       "                                                   max_leaves=None,\n",
       "                                                   min_child_weight=None,\n",
       "                                                   missing=nan,\n",
       "                                                   monotone_constraints=None,\n",
       "                                                   multi_strategy=None,\n",
       "                                                   n_estimators=None,\n",
       "                                                   n_jobs=None,\n",
       "                                                   num_parallel_tree=None, ...)],\n",
       "                          &#x27;models__learning_rate&#x27;: [0.3, 0.2, 0.1, 0.4],\n",
       "                          &#x27;models__max_depth&#x27;: [4],\n",
       "                          &#x27;models__n_estimators&#x27;: [500, 1000, 1250]}],\n",
       "             scoring=make_scorer(f1_eval, response_method=&#x27;predict&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;models&#x27;,\n",
       "                                        [LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                            random_state=88)])]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;models&#x27;: [LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                        random_state=88)],\n",
       "                          &#x27;models__solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;liblinear&#x27;]},\n",
       "                         {&#x27;models&#x27;: [XGBClassifier(base_score=None,\n",
       "                                                   booster=None, callbacks=None,\n",
       "                                                   colsample_bylevel=None,\n",
       "                                                   c...\n",
       "                                                   max_delta_step=None,\n",
       "                                                   max_depth=None,\n",
       "                                                   max_leaves=None,\n",
       "                                                   min_child_weight=None,\n",
       "                                                   missing=nan,\n",
       "                                                   monotone_constraints=None,\n",
       "                                                   multi_strategy=None,\n",
       "                                                   n_estimators=None,\n",
       "                                                   n_jobs=None,\n",
       "                                                   num_parallel_tree=None, ...)],\n",
       "                          &#x27;models__learning_rate&#x27;: [0.3, 0.2, 0.1, 0.4],\n",
       "                          &#x27;models__max_depth&#x27;: [4],\n",
       "                          &#x27;models__n_estimators&#x27;: [500, 1000, 1250]}],\n",
       "             scoring=make_scorer(f1_eval, response_method=&#x27;predict&#x27;))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;models&#x27;,\n",
       "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, device=None,\n",
       "                               early_stopping_rounds=None,\n",
       "                               enable_categorical=False,\n",
       "                               eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "                               feature_types=None, feature_weights=None,\n",
       "                               gamma=None, grow_policy=None,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.2,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=4, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, multi_strategy=None,\n",
       "                               n_estimators=500, n_jobs=None,\n",
       "                               num_parallel_tree=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('models',\n",
       "                                        [LogisticRegression(class_weight='balanced',\n",
       "                                                            random_state=88)])]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'models': [LogisticRegression(class_weight='balanced',\n",
       "                                                        random_state=88)],\n",
       "                          'models__solver': ['newton-cg', 'liblinear']},\n",
       "                         {'models': [XGBClassifier(base_score=None,\n",
       "                                                   booster=None, callbacks=None,\n",
       "                                                   colsample_bylevel=None,\n",
       "                                                   c...\n",
       "                                                   max_delta_step=None,\n",
       "                                                   max_depth=None,\n",
       "                                                   max_leaves=None,\n",
       "                                                   min_child_weight=None,\n",
       "                                                   missing=nan,\n",
       "                                                   monotone_constraints=None,\n",
       "                                                   multi_strategy=None,\n",
       "                                                   n_estimators=None,\n",
       "                                                   n_jobs=None,\n",
       "                                                   num_parallel_tree=None, ...)],\n",
       "                          'models__learning_rate': [0.3, 0.2, 0.1, 0.4],\n",
       "                          'models__max_depth': [4],\n",
       "                          'models__n_estimators': [500, 1000, 1250]}],\n",
       "             scoring=make_scorer(f1_eval, response_method='predict'))"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipe, \n",
    "    param_grid, \n",
    "    cv=5,\n",
    "    #scoring='f1',\n",
    "    scoring=my_func,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(dembs_train, y_train)\n",
    "#grid_search.fit(trial, y_train.values)\n",
    "#grid_search.fit(trial, res_samp['toxic'])\n",
    "###grid_search.fit(tr.drop('toxic', axis=1), tr['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators \t2000, learning_rate \t0.1, max_depth \t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.712537</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 500}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.709485</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 1250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.708585</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 500}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.706557</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.706455</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 1250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>0.702906</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 1250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.700879</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 500}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.700150</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>0.699740</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 1250}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.699395</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 500}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.699390</td>\n",
       "      <td>{'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function f1_xgb at 0x000002541A5DECA0&gt;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.668887</td>\n",
       "      <td>{'models': LogisticRegression(class_weight='balanced', random_state=88), 'models__solver': 'liblinear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.668887</td>\n",
       "      <td>{'models': LogisticRegression(class_weight='balanced', random_state=88), 'models__solver': 'newton-cg'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  mean_test_score  \\\n",
       "5                 1         0.712537   \n",
       "6                 2         0.710526   \n",
       "7                 3         0.709485   \n",
       "2                 4         0.708585   \n",
       "3                 5         0.706557   \n",
       "4                 6         0.706455   \n",
       "10                7         0.702906   \n",
       "8                 8         0.700879   \n",
       "9                 9         0.700150   \n",
       "13               10         0.699740   \n",
       "11               11         0.699395   \n",
       "12               12         0.699390   \n",
       "1                13         0.668887   \n",
       "0                13         0.668887   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     params  \n",
       "5    {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 500}  \n",
       "6   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 1000}  \n",
       "7   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.2, 'models__max_depth': 4, 'models__n_estimators': 1250}  \n",
       "2    {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 500}  \n",
       "3   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 1000}  \n",
       "4   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.3, 'models__max_depth': 4, 'models__n_estimators': 1250}  \n",
       "10  {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 1250}  \n",
       "8    {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 500}  \n",
       "9   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.1, 'models__max_depth': 4, 'models__n_estimators': 1000}  \n",
       "13  {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 1250}  \n",
       "11   {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 500}  \n",
       "12  {'models': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=<function f1_xgb at 0x000002541A5DECA0>,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...), 'models__learning_rate': 0.4, 'models__max_depth': 4, 'models__n_estimators': 1000}  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'models': LogisticRegression(class_weight='balanced', random_state=88), 'models__solver': 'liblinear'}  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'models': LogisticRegression(class_weight='balanced', random_state=88), 'models__solver': 'newton-cg'}  "
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'mean_test_score','params']].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'mean_test_score','params']].sort_values('rank_test_score').iloc[2]['params']#.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grid_pred = grid_search.best_estimator_.predict(debs_no_dups_test.drop('toxic', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_pred = grid_search.best_estimator_.predict(trial_trial)\n",
    "grid_pred = grid_search.best_estimator_.predict(dembs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96988348, 0.71711595])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(grid_pred, y_test,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     14572\n",
      "           1       0.67      0.77      0.72      1428\n",
      "\n",
      "    accuracy                           0.95     16000\n",
      "   macro avg       0.82      0.87      0.84     16000\n",
      "weighted avg       0.95      0.95      0.95     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_pred, y_test))\n",
    "#print(classification_report(grid_pred, test_samp['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier_tuned_threshold = TunedThresholdClassifierCV(\n",
    "    grid_search.best_estimator_, scoring=my_func, n_jobs=-1\n",
    ").fit(dembs_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctt_pred = classifier_tuned_threshold.predict(dembs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.2020202)"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_tuned_threshold.best_threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7221570926143025"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, ctt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722735674676525"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_other_threshold = FixedThresholdClassifier(\n",
    "    grid_search.best_estimator_, threshold=0.35, response_method=\"predict_proba\"\n",
    ").fit(dembs_train, y_train)\n",
    "\n",
    "cot_pred = classifier_other_threshold.predict(dembs_test)\n",
    "\n",
    "f1_score(y_test, cot_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_other_threshold = FixedThresholdClassifier(\n",
    "    grid_search.best_estimator_, threshold=0.6, response_method=\"predict_proba\"\n",
    ").fit(debs_no_dups.drop('toxic', axis=1), debs_no_dups['toxic'])\n",
    "f1_score(debs_no_dups_test['toxic'], classifier_other_threshold.predict(debs_no_dups_test.drop('toxic', axis=1)))#,average=None)#82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dembs_train).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, classifier_other_threshold.predict(dembs_test))#82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_preds_adj = classifier_tuned.predict(dembs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(cl_preds_adj,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =pd.DataFrame({'a':grid_pred, 'b':test_sample['toxic']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'proba':list(probs[1])},\n",
    "              {'predicted':grid_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_res = pd.DataFrame(zip(y_test,probs[1],grid_pred),columns=['toxic', 'proba', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_res[(probs_res['toxic']!=probs_res['pred']&(probs_res['toxic']==1))]['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[1961]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.apply(lambda x: len(x))>2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.index.isin(res[res['a']!=res['b']].index))&(df['toxic']==1)]['text'].apply(lambda x: tokenizer.decode(chunker(x)))#.sample()\n",
    "\n",
    "\n",
    "\n",
    "#['b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(chunker(df.loc[128933]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[128933]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from phik.report import plot_correlation_matrix\n",
    "#from phik import phik_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phik_overview = phik_matrix(df[['toxic','word_count']],verbose=False)\n",
    "\n",
    "plot_correlation_matrix(\n",
    "    phik_overview.values,\n",
    "    x_labels=phik_overview.columns,\n",
    "    y_labels=phik_overview.index,\n",
    "    vmin=0, vmax=1, color_map='Blues',\n",
    "    title=r'correlation $\\phi_K$',\n",
    "    fontsize_factor=1.5,\n",
    "    figsize=(18, 14)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_lemm'], \n",
    "    df['toxic'], \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['toxic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_resample, y_resample = sampler.fit_resample(pd.DataFrame(X_train), pd.DataFrame(y_train)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resample.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 406,
    "start_time": "2025-08-18T19:09:24.283Z"
   },
   {
    "duration": 976,
    "start_time": "2025-08-18T19:09:47.447Z"
   },
   {
    "duration": 13,
    "start_time": "2025-08-18T19:09:57.004Z"
   },
   {
    "duration": 34,
    "start_time": "2025-08-18T19:11:01.787Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:11:51.371Z"
   },
   {
    "duration": 71,
    "start_time": "2025-08-18T19:18:11.221Z"
   },
   {
    "duration": 7324,
    "start_time": "2025-08-18T19:18:31.752Z"
   },
   {
    "duration": 2366,
    "start_time": "2025-08-18T19:18:42.522Z"
   },
   {
    "duration": 638,
    "start_time": "2025-08-18T19:18:46.740Z"
   },
   {
    "duration": 52,
    "start_time": "2025-08-18T19:19:05.932Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T19:20:05.266Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T19:21:09.819Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:21:24.068Z"
   },
   {
    "duration": 23,
    "start_time": "2025-08-18T19:22:05.917Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:23:17.506Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T19:24:15.955Z"
   },
   {
    "duration": 99,
    "start_time": "2025-08-18T19:24:30.236Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T19:24:50.180Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T19:25:21.649Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T19:25:35.470Z"
   },
   {
    "duration": 54,
    "start_time": "2025-08-18T19:27:41.878Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:27:55.628Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T19:28:25.512Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T19:28:36.733Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T19:43:50.418Z"
   },
   {
    "duration": 76,
    "start_time": "2025-08-18T19:43:53.290Z"
   },
   {
    "duration": 44,
    "start_time": "2025-08-18T19:43:58.942Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:01:42.440Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:04:30.308Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:08:23.893Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:08:51.300Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:09:05.354Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:09:48.303Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:10:01.688Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:11:33.427Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:11:38.262Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:11:39.629Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:11:50.462Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T20:11:51.853Z"
   },
   {
    "duration": 4391,
    "start_time": "2025-08-18T20:12:50.783Z"
   },
   {
    "duration": 4355,
    "start_time": "2025-08-18T20:13:13.006Z"
   },
   {
    "duration": 4625,
    "start_time": "2025-08-18T20:13:32.152Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:17:06.253Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T20:30:29.649Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:30:37.731Z"
   },
   {
    "duration": 3208,
    "start_time": "2025-08-18T20:30:40.953Z"
   },
   {
    "duration": 208,
    "start_time": "2025-08-18T20:31:13.163Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:31:43.147Z"
   },
   {
    "duration": 25,
    "start_time": "2025-08-18T20:32:15.848Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:32:37.920Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:32:47.420Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:35:57.628Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T20:38:26.028Z"
   },
   {
    "duration": 84,
    "start_time": "2025-08-18T20:41:50.277Z"
   },
   {
    "duration": 500,
    "start_time": "2025-08-18T20:42:03.816Z"
   },
   {
    "duration": 30,
    "start_time": "2025-08-18T20:42:27.568Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:42:43.879Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:42:52.612Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T20:43:10.466Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:43:34.481Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-18T20:45:39.302Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:46:42.581Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:46:46.700Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:47:07.407Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:48:23.658Z"
   },
   {
    "duration": 14,
    "start_time": "2025-08-18T20:48:42.988Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:49:12.478Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-18T20:49:21.311Z"
   },
   {
    "duration": 82,
    "start_time": "2025-08-18T20:50:04.814Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T20:50:41.375Z"
   },
   {
    "duration": 72,
    "start_time": "2025-08-18T20:50:44.499Z"
   },
   {
    "duration": 18,
    "start_time": "2025-08-18T20:51:17.776Z"
   },
   {
    "duration": 31,
    "start_time": "2025-08-18T20:51:32.035Z"
   },
   {
    "duration": 33,
    "start_time": "2025-08-18T20:52:03.189Z"
   },
   {
    "duration": 70,
    "start_time": "2025-08-18T20:53:02.892Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:53:17.975Z"
   },
   {
    "duration": 10,
    "start_time": "2025-08-18T20:53:35.266Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:53:46.379Z"
   },
   {
    "duration": 69,
    "start_time": "2025-08-18T20:53:47.797Z"
   },
   {
    "duration": 302,
    "start_time": "2025-08-18T20:54:14.768Z"
   },
   {
    "duration": 375,
    "start_time": "2025-08-18T20:54:27.606Z"
   },
   {
    "duration": 70,
    "start_time": "2025-08-18T20:54:33.372Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T20:54:52.299Z"
   },
   {
    "duration": 1172,
    "start_time": "2025-08-18T20:55:36.234Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:55:47.977Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-18T20:55:54.214Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T20:56:35.643Z"
   },
   {
    "duration": 17,
    "start_time": "2025-08-18T20:56:51.987Z"
   },
   {
    "duration": 320,
    "start_time": "2025-08-18T20:59:09.313Z"
   },
   {
    "duration": 8087,
    "start_time": "2025-08-18T20:59:46.419Z"
   },
   {
    "duration": 488,
    "start_time": "2025-08-18T21:00:10.574Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T21:00:32.098Z"
   },
   {
    "duration": 495,
    "start_time": "2025-08-18T21:00:33.903Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:00:36.626Z"
   },
   {
    "duration": 12,
    "start_time": "2025-08-18T21:00:37.887Z"
   },
   {
    "duration": 30,
    "start_time": "2025-08-18T21:00:41.452Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:00:57.292Z"
   },
   {
    "duration": 24,
    "start_time": "2025-08-18T21:01:12.068Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:07:00.983Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T21:07:27.166Z"
   },
   {
    "duration": 2,
    "start_time": "2025-08-18T21:07:28.929Z"
   },
   {
    "duration": 769,
    "start_time": "2025-08-18T21:07:48.911Z"
   },
   {
    "duration": 84,
    "start_time": "2025-08-18T21:08:43.198Z"
   },
   {
    "duration": 67,
    "start_time": "2025-08-18T21:11:21.466Z"
   },
   {
    "duration": 13733,
    "start_time": "2025-08-18T21:12:07.873Z"
   },
   {
    "duration": 357052,
    "start_time": "2025-08-18T21:12:26.388Z"
   },
   {
    "duration": 2420,
    "start_time": "2025-08-18T21:18:38.380Z"
   },
   {
    "duration": 1036,
    "start_time": "2025-08-18T21:18:40.802Z"
   },
   {
    "duration": 2780,
    "start_time": "2025-08-18T21:18:41.839Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:44.621Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T21:18:44.626Z"
   },
   {
    "duration": 873,
    "start_time": "2025-08-18T21:18:44.635Z"
   },
   {
    "duration": 16,
    "start_time": "2025-08-18T21:18:45.510Z"
   },
   {
    "duration": 50,
    "start_time": "2025-08-18T21:18:45.527Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:45.579Z"
   },
   {
    "duration": 4422,
    "start_time": "2025-08-18T21:18:45.584Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-18T21:18:50.007Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-18T21:18:50.015Z"
   },
   {
    "duration": 651,
    "start_time": "2025-08-18T21:18:50.028Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-18T21:18:50.680Z"
   },
   {
    "duration": 8,
    "start_time": "2025-08-18T21:18:50.684Z"
   },
   {
    "duration": 38,
    "start_time": "2025-08-18T21:18:50.694Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-18T21:18:50.734Z"
   },
   {
    "duration": 7,
    "start_time": "2025-08-18T21:18:50.741Z"
   },
   {
    "duration": 97352,
    "start_time": "2025-08-18T21:18:50.749Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.102Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.104Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.105Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.106Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-18T21:20:28.107Z"
   },
   {
    "duration": 55,
    "start_time": "2025-08-19T05:57:12.644Z"
   },
   {
    "duration": 1364,
    "start_time": "2025-08-19T05:57:18.742Z"
   },
   {
    "duration": 9004,
    "start_time": "2025-08-19T05:57:22.652Z"
   },
   {
    "duration": 25,
    "start_time": "2025-08-19T05:57:31.660Z"
   },
   {
    "duration": 2845,
    "start_time": "2025-08-19T05:57:59.992Z"
   },
   {
    "duration": 1205,
    "start_time": "2025-08-19T05:58:02.840Z"
   },
   {
    "duration": 3666,
    "start_time": "2025-08-19T05:58:04.048Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:07.716Z"
   },
   {
    "duration": 53,
    "start_time": "2025-08-19T05:58:07.721Z"
   },
   {
    "duration": 1056,
    "start_time": "2025-08-19T05:58:07.776Z"
   },
   {
    "duration": 28,
    "start_time": "2025-08-19T05:58:08.834Z"
   },
   {
    "duration": 79,
    "start_time": "2025-08-19T05:58:08.864Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-19T05:58:08.945Z"
   },
   {
    "duration": 4967,
    "start_time": "2025-08-19T05:58:08.951Z"
   },
   {
    "duration": 8,
    "start_time": "2025-08-19T05:58:13.919Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:13.929Z"
   },
   {
    "duration": 771,
    "start_time": "2025-08-19T05:58:13.934Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T05:58:14.708Z"
   },
   {
    "duration": 22,
    "start_time": "2025-08-19T05:58:14.713Z"
   },
   {
    "duration": 23175,
    "start_time": "2025-08-19T05:58:14.737Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.915Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.916Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.917Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.919Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.921Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T05:58:37.922Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T06:00:31.236Z"
   },
   {
    "duration": 2924,
    "start_time": "2025-08-19T06:01:01.130Z"
   },
   {
    "duration": 1170,
    "start_time": "2025-08-19T06:01:04.058Z"
   },
   {
    "duration": 3433,
    "start_time": "2025-08-19T06:01:05.230Z"
   },
   {
    "duration": 4,
    "start_time": "2025-08-19T06:01:08.665Z"
   },
   {
    "duration": 802,
    "start_time": "2025-08-19T06:01:08.672Z"
   },
   {
    "duration": 3,
    "start_time": "2025-08-19T06:01:09.476Z"
   },
   {
    "duration": 1034,
    "start_time": "2025-08-19T06:01:09.481Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-19T06:01:10.517Z"
   },
   {
    "duration": 56,
    "start_time": "2025-08-19T06:01:10.538Z"
   },
   {
    "duration": 69,
    "start_time": "2025-08-19T06:01:10.595Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-19T06:01:10.666Z"
   },
   {
    "duration": 6,
    "start_time": "2025-08-19T06:01:10.674Z"
   },
   {
    "duration": 19,
    "start_time": "2025-08-19T06:01:10.682Z"
   },
   {
    "duration": 3056180,
    "start_time": "2025-08-19T06:01:10.704Z"
   },
   {
    "duration": 9,
    "start_time": "2025-08-19T06:52:06.886Z"
   },
   {
    "duration": 37,
    "start_time": "2025-08-19T06:52:06.898Z"
   },
   {
    "duration": 345,
    "start_time": "2025-08-19T06:52:06.937Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.284Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.286Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.287Z"
   },
   {
    "duration": 0,
    "start_time": "2025-08-19T06:52:07.289Z"
   },
   {
    "duration": 26,
    "start_time": "2025-08-19T06:54:01.622Z"
   },
   {
    "duration": 98,
    "start_time": "2025-08-19T06:54:07.568Z"
   },
   {
    "duration": 273,
    "start_time": "2025-08-19T06:56:24.310Z"
   },
   {
    "duration": 47,
    "start_time": "2025-08-19T06:56:29.808Z"
   },
   {
    "duration": 26,
    "start_time": "2025-08-19T06:56:54.409Z"
   },
   {
    "duration": 5,
    "start_time": "2025-08-19T07:00:03.035Z"
   },
   {
    "duration": 11,
    "start_time": "2025-08-19T07:00:07.705Z"
   },
   {
    "duration": 43,
    "start_time": "2025-08-19T07:01:09.198Z"
   },
   {
    "duration": 22,
    "start_time": "2025-08-19T07:01:17.084Z"
   },
   {
    "duration": 14,
    "start_time": "2025-08-19T07:01:20.949Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
